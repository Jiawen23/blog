##############################################
双重意图和实体 Transformer（DIET）
##############################################


在 ``FAQ`` 问答机器人中，通常有两种意图识别的方法，一种是基于文本相似的，另一种是基于多分类模型的。
这种两种方法并不是互斥的，很多时候可以配合着一起用。


- 基于相似的。计算用的 query 和问答库中所有 question（标题、相似问法、解答文本） 的相似度，选取最相似的结果作为用户意图。
- 基于多分类的。问答库中每一个 question 对应一个类别，把用户的输入 query 喂给一个多分类模型，分类结果作为用户意图。

``rasa`` 默认采用的是第二种，并且提供了多种分类模型的支持，目前有：``MitieIntentClassifier``,
``SklearnIntentClassifier(SVM)``, ``KeywordIntentClassifier``, ``DIETClassifier``
几种分类模型，并且支持自定义新的模型组件。
其中比较特殊的模型是 ``DIETClassifier``，
它是 ``rasa`` 项目组自研的一个多任务模型，
同时支持实体识别和意图分类。
::
    DIET (Dual Intent and Entity Transformer) is a multi-task architecture
    for intent classification and entity recognition.
    The architecture is based on a transformer which is shared for both tasks.
    A sequence of entity labels is predicted through a Conditional Random Field (CRF)
    tagging layer on top of the transformer output sequence corresponding to the input sequence of tokens.
    For the intent labels the transformer output for the complete utterance
    and intent labels are embedded into a single semantic vector space.
    We use the dot-product loss to maximize the similarity with the target label
    and minimize similarities with negative samples.




模型输入
######################################


每一条 ``Text`` 有两种特征：``Sequence Feature`` 和 ``Sentence Feature``。

:``Sequence Feature``:
    每条 ``Text`` 分词后得到一个 ``token`` 的序列，
    所有 ``token`` 的特征形成的序列就是当前 ``Text`` 的 ``Sequence Feature``。
    ``Sequence Feature=List[Token Feature]``

        The sequence features are a matrix of size ``(number-of-tokens x feature-dimension)``.
        The matrix contains a feature vector for every token in the sequence.
        This allows us to train sequence models.

:``Sentence Feature``:
    当前 ``Text`` 的整体特征，比如句子向量、是否包含特征信息等等，描述整个句子的特征。

        The sentence features are represented by a matrix of size ``(1 x feature-dimension)``.
        It contains the feature vector for the complete utterance.
        The sentence features can be used in any bag-of-words model

        The corresponding classifier can therefore decide what kind of features to use.
        Note: The feature-dimension for sequence and sentence features does not have to be the same.


无论是 ``Token(Sequence) Feature`` 还是 ``Sentence Feature`` 又分成两种具体的特征类型：
``SparseFeature`` 和 ``DenseFeature``。

:``SparseFeature``:
    稀疏特征，或者离散特征，比如 ``one-hot`` 特征。
    对于 ``Token Feature`` ，它的 ``SparseFeature`` 一般是它的 ``one-hot`` 向量。
    对于 ``Sentence Feature`` ，它的 ``SparseFeature`` 可以是："句子中是否包含某个信息"，
    这类二值特征。

:``DenseFeature``:
    稠密特征。
    对于 ``Token Feature`` ，它的 ``DenseFeature`` 一般是它 **词向量**。
    对于 ``Sentence Feature`` ，它的 ``DenseFeature`` 可以是它 **句子向量**。




网络层
######################################





DenseWithSparseWeights
==========================================




**本身是一个全连接层，区别就是它的输入是稀疏矩阵**。
在此基础上增加了一个额外的小功能。
可以按照固定比例，把权重矩阵部分值设置为0.0，
由入参 ``sparsity`` 控制这个比例。


:class:`rasa.utils.tensorflow.layers.DenseWithSparseWeights`


.. code-block:: python

    class DenseWithSparseWeights(tf.keras.layers.Dense):
        """
        本身是一个全连接层，在此基础上增加了一个额外的小功能。
        可以按照固定比例，把权重矩阵部分值设置为0.0，
        由入参 ``sparsity`` 控制这个比例。

        Just your regular densely-connected NN layer but with sparse weights.

        `Dense` implements the operation:
        `output = activation(dot(input, kernel) + bias)`
        where `activation` is the element-wise activation function
        passed as the `activation` argument, `kernel` is a weights matrix
        created by the layer, and `bias` is a bias vector created by the layer
        (only applicable if `use_bias` is `True`).
        It creates `kernel_mask` to set fraction of the `kernel` weights to zero.

        Note: If the input to the layer has a rank greater than 2, then
        it is flattened prior to the initial dot product with `kernel`.

        Arguments:
            sparsity: Float between 0 and 1. Fraction of the `kernel`
                weights to set to zero.
            units: Positive integer, dimensionality of the output space.
            activation: Activation function to use.
                If you don't specify anything, no activation is applied
                (ie. "linear" activation: `a(x) = x`).
            use_bias: Boolean, whether the layer uses a bias vector.
            kernel_initializer: Initializer for the `kernel` weights matrix.
            bias_initializer: Initializer for the bias vector.
            kernel_regularizer: Regularizer function applied to
                the `kernel` weights matrix.
            bias_regularizer: Regularizer function applied to the bias vector.
            activity_regularizer: Regularizer function applied to
                the output of the layer (its "activation")..
            kernel_constraint: Constraint function applied to
                the `kernel` weights matrix.
            bias_constraint: Constraint function applied to the bias vector.

        Input shape:
            N-D tensor with shape: `(batch_size, ..., input_dim)`.
            The most common situation would be
            a 2D input with shape `(batch_size, input_dim)`.

        Output shape:
            N-D tensor with shape: `(batch_size, ..., units)`.
            For instance, for a 2D input with shape `(batch_size, input_dim)`,
            the output would have shape `(batch_size, units)`.
        """

        def __init__(self, sparsity: float = 0.8, **kwargs: Any) -> None:
            super().__init__(**kwargs)
            self.sparsity = sparsity

        def build(self, input_shape: tf.TensorShape) -> None:
            super().build(input_shape)
            # create random mask to set fraction of the `kernel` weights to zero
            # 按照均匀分布生成 0~1 之间的随机值
            kernel_mask = tf.random.uniform(tf.shape(self.kernel), 0, 1)

            # bool 矩阵强制转换成 self.kernel.dtype 类型
            # True -> 1.0
            # False -> 0.0
            kernel_mask = tf.cast(
                # 返回一个bool矩阵：
                # 大于等于 self.sparsity 的位置是 True，反之是 False
                tf.greater_equal(kernel_mask, self.sparsity), self.kernel.dtype
            )
            # kernel_mask 转换成一个张量变量，注意：trainable=False 表示它的常量值
            self.kernel_mask = tf.Variable(
                initial_value=kernel_mask, trainable=False, name="kernel_mask"
            )

        def call(self, inputs: tf.Tensor) -> tf.Tensor:
            # set fraction of the `kernel` weights to zero according to precomputed mask
            # 把权重 self.kernel 中部分位置设置为 0.0  inputs.shape=(N_samples,N_tokens,512)  kernel.shape=(512,units)
            self.kernel.assign(self.kernel * self.kernel_mask)
            return super().call(inputs)






前馈（Feed-Forward）网络单元
==============================================




前向网络层（Feed-forward network layer）。

2层 DenseWithSparseWeights + 1 层 dropout


:class:`rasa.utils.tensorflow.layers.Ffnn`

.. digraph:: FFNN

    graph [rankdir=TB, clusterrank="local",fontsize=20];
    fillcolor="#80808018";

        label="FFNN\n前馈网络";
        style="filled,rounded";

        inputs [label="Inputs", style="filled", shape="box",fillcolor="#80808018"];
        layer1 [label="DenseWithSparseWeights\n(所有token的特征共享本层权重)", style="filled", shape="box",fillcolor="#80808018"];
        layer2 [label="DenseWithSparseWeights\n(所有token的特征共享本层权重)", style="filled", shape="box", fillcolor="#80808018"];
        layer3 [label="Dropout", style="filled", shape="box", fillcolor="#80808018"];

        inputs -> layer1;
        layer1 -> layer2;
        layer2 -> layer3;


模型网络结构
##############################################

输入数据
==============================================



.. code-block:: python

    data_signature = {
        'label': {
            'ids': [FeatureSignature(is_sparse=False, units=1, number_of_dimensions=2)],
            'mask': [FeatureSignature(is_sparse=False, units=1, number_of_dimensions=3)],
            'sequence': [FeatureSignature(is_sparse=True, units=381, number_of_dimensions=3)],
            'sequence_lengths': [
                FeatureSignature(is_sparse=False, units=11478, number_of_dimensions=1)]
        },
        'text': {
            'mask': [FeatureSignature(is_sparse=False, units=1, number_of_dimensions=3)],
            'sentence': [FeatureSignature(is_sparse=True, units=31789, number_of_dimensions=3)],
            'sequence': [FeatureSignature(is_sparse=True, units=31813, number_of_dimensions=3)],
            'sequence_lengths': [
                FeatureSignature(is_sparse=False, units=11478, number_of_dimensions=1)]
        }
    }


    predict_data_signature = {
    'text': DictWrapper(
        {'mask': ListWrapper([FeatureSignature(is_sparse=False, units=1, number_of_dimensions=3)]),
         'sentence': ListWrapper([FeatureSignature(is_sparse=True, units=31789, number_of_dimensions=3)]),
         'sequence': ListWrapper([FeatureSignature(is_sparse=True, units=31813, number_of_dimensions=3)]),
         'sequence_lengths': ListWrapper([FeatureSignature(is_sparse=False, units=11478, number_of_dimensions=1)])
         })
    }




.. code-block:: python

    # batch_to_model_data_format 函数返回的结果
    tf_batch_data = {
        "label": {"ids": List[Tensor(None, 1)],
                  "mask": List[Tensor(None, None, 1)],
                  "sequence": List[SparseTensor,DenseTensor],
                  "sequence_lengths": [Tensor(None, )],
                  "sentence": List[SparseTensor,DenseTensor],
                  },
        "text":
            {
                # 目前是猜测，还没准确确定
                # 每条样本是一个对话，对话中包含多条句子，有的句子没有抽取到特征。
                # mask 用于标记对话中哪些句子有特征，哪些句子没有特征（没有意义）。
                # 模型输入的时候，应该会仅保留有特征（有意义）的句子
                # 虽然是 List，但List只有一个元素
                "mask": List[Tensor(None, None, 1)], # shape=(N_samples,N_text,1)

                # 句子的特征
                # SparseTensor 是稀疏特征
                # DenseTensor 是稠密特征
                # shape=(N_samples,1,N_features)
                "sentence": List[SparseTensor,DenseTensor],

                # 样本的 token 序列特征，
                # SparseTensor 是稀疏特征
                # DenseTensor 是稠密特征
                # shape=(N_samples,Max(N_tokens),N_sparse_features)
                "sequence": List[SparseTensor,DenseTensor],

                # 每条样本的 token长度，
                # shape=(N_samples,)
                # 虽然是 List，但List只有一个元素
                "sequence_lengths": List[Tensor(None, )],


            }
    }






特征合并单元
================================================


``SparseFeature`` 和 ``DenseFeature`` **特征合并单元：**

每个 token 的特征分为 ``SparseFeature[SparseTensor]`` 和 ``DenseFeature[SparseTensor]`` 两种，
特征合并单元负责将两种特征拼接在一起。

.. digraph:: combine

    graph [rankdir=TB, clusterrank="local",fontsize=20];

        label="Spars Dense 合并";
        style="filled,rounded";


        input_sparse [label="Input[SparseTensor]", style="filled", shape="box", fillcolor="#80808018"];
        input_dense [label="Input[DenseTensor]", style="filled", shape="box",fillcolor="#80808018"];
        SparseDropout [label="SparseDropout", style="filled", shape="box",fillcolor="#80808018"];
        DenseForSparse [label="DenseForSparse\n支持SparseTensor的全连接层", style="filled", shape="box",fillcolor="#80808018" ];
        Dropout [label="Dropout", style="filled", shape="box",fillcolor="#80808018"];
        concat [label="Concat", style="filled", shape="box",fillcolor="#80808018"];

        { rank = same; input_sparse,input_dense}

        input_sparse -> SparseDropout -> DenseForSparse->Dropout;
        Dropout -> concat;
        input_dense -> concat;










完整网络图结构
==========================================================




.. digraph:: DIET

    graph [rankdir=TB, clusterrank="local",fontsize=20];

    compound=true;

    subgraph cluster_play {

            graph [style="filled,rounded", fillcolor="#80808018", label="play"];
            play_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box",];
            play_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box"];
    }

    subgraph cluster_ping {
            label="ping";
            style="filled,rounded";
            fillcolor="#80808018";
            ping_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            ping_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];

    }
    subgraph cluster_pong {
            label="pong";
            style="filled,rounded";
            fillcolor="#80808018";
            pong_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            pong_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];

    }
    subgraph cluster_cls {
            label="_CLS_[sentence]";
            style="filled,rounded";
            fillcolor="pink";
            cls_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            cls_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];

    }



    combine_sparse_dense_text [label="Sparse Dense\n拼接单元", style="filled", fillcolor="lightcyan", fontcolor="#000000",  shape="box",width="5"];


    //{ping_sparse,ping_dense,pong_sparse,pong_dense,cls_sparse,cls_dense} -> combine_sparse_dense_text

    play_sparse -> combine_sparse_dense_text [ltail="cluster_play"];
    ping_sparse -> combine_sparse_dense_text [ltail="cluster_ping"];
    pong_sparse -> combine_sparse_dense_text [ltail="cluster_pong"];
    cls_sparse -> combine_sparse_dense_text [ltail="cluster_cls"];


    tmp0_play [label="play", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp0_ping [label="ping", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp0_pong [label="pong", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp0_cls [label="_CLS_", shape="box",style="filled,rounded,dashed",fillcolor="lightpink"]


    combine_sparse_dense_text->{tmp0_play,tmp0_ping,tmp0_pong,tmp0_cls}
    {tmp0_play,tmp0_ping,tmp0_pong,tmp0_cls}->FFNN_text



    FFNN_text [label="FFNN_Text", style="filled", fillcolor="thistle", fontcolor="#000000",  shape="box",width="5"];

    tmp1_play [label="play", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp1_ping [label="ping", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp1_pong [label="pong", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp1_cls [label="_CLS_", shape="box",style="filled,rounded,dashed",fillcolor="lightpink"]

    mask [label="Sequence Mask\n随机遮挡15%的token", shape="box",style="filled,", fillcolor="lightskyblue",width="5"]




    FFNN_text -> {tmp1_play,tmp1_ping,tmp1_pong,tmp1_cls}
    {tmp1_play,tmp1_ping,tmp1_pong,tmp1_cls} -> mask;

    // 强制 顺序
    { rank = same; tmp1_play -> tmp1_ping -> tmp1_pong -> tmp1_cls[ style = invis ] }


    tmp2_mask [label="_MASK_\n pong", shape="box", style="filled,rounded,dashed",fillcolor=yellow,]
    tmp2_cls [label="_CLS_", shape="box", style="filled,rounded,dashed",fillcolor=pink,]
    tmp2_play [label="play", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp2_ping [label="ping", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]

    { rank = same; tmp2_play -> tmp2_ping -> tmp2_mask -> tmp2_cls[ style = invis ] }

    mask -> tmp2_play
    mask -> tmp2_ping
    mask -> tmp2_mask
    mask -> tmp2_cls

    transformer [label="Transformer(2 layers)", shape="box",style="filled,", fillcolor="lightgreen",width="5"]
    {tmp2_play,tmp2_ping,tmp2_mask,tmp2_cls} -> transformer


    tmp3_mask [label="_MASK_\n pong", shape="box", style="filled,rounded,dashed",fillcolor=yellow,]
    tmp3_cls [label="_CLS_", shape="box", style="filled,rounded,dashed",fillcolor=pink,]
    tmp3_play [label="play", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]
    tmp3_ping [label="ping", shape="box",style="filled,rounded,dashed",fillcolor="#80808018",]

    { rank = same; tmp3_play -> tmp3_ping -> tmp3_mask -> tmp3_cls[ style = invis ] }

    transformer-> {tmp3_play,tmp3_ping,tmp3_mask,tmp3_cls}

    // 开始：Mask Loss
    Loss_lm_mask  [label="LM Mask Loss \nSimilarity", style="filled", fillcolor="tomato", shape="box"]

    {tmp1_pong,tmp3_mask} -> Loss_lm_mask ;


    // 开始：标签处理
    subgraph cluster_label {
        label="Label[Intent]";
        ordering=out;

        subgraph cluster_label_token_1 {
            label="_CLS_[sentence]";
            style="filled,rounded";
            fillcolor="pink";
            label_token1_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            label_token1_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];
        }
        subgraph cluster_label_token_2 {
            label="token2";
            style="filled,rounded";
            fillcolor="pink";
            label_token2_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            label_token2_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];
        }
        subgraph cluster_label_token_3 {
            label="token1";
            style="filled,rounded";
            fillcolor="pink";
            label_token3_sparse [label="Sparse\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000",  shape="box",group="0"];
            label_token3_dense [label="Dense\nfeatures", style="filled", fillcolor="#ffffffb2", fontcolor="#000000", shape="box", group="0"];
        }
    }
    //{rank=same;label_token1_sparse->label_token1_dense->label_token2_sparse->label_token2_dense->label_token3_sparse->label_token3_dense[ style = invis ]}

    combine_sparse_dense_label [label="Sparse Dense\n拼接单元", style="filled", fillcolor="lightcyan", fontcolor="#000000",  shape="box",width="4"];

     {label_token1_sparse} -> combine_sparse_dense_label [ltail=cluster_label_token_1]
     {label_token2_sparse} -> combine_sparse_dense_label [ltail=cluster_label_token_2]
     {label_token3_sparse} -> combine_sparse_dense_label [ltail=cluster_label_token_3]

    FFNN_label [label="FFNN_Label", style="filled", fillcolor="thistle", fontcolor="#000000",  shape="box",width="4"];

    combine_sparse_dense_label  -> FFNN_label
    combine_sparse_dense_label  -> FFNN_label
    combine_sparse_dense_label  -> FFNN_label
    combine_sparse_dense_label  -> FFNN_label

    label_token_sum [label="Reduce sum by token sequence", style="filled", fillcolor="peru", fontcolor="#000000", shape="box", group="0"];
    // 结束：标签处理

    // 开始：意图识别损失
    Loss_intent  [label="Intent Loss\n Similarity", style="filled", fillcolor="tomato", shape="box"]

    FFNN_label ->  label_token_sum
    FFNN_label ->  label_token_sum
    FFNN_label ->  label_token_sum
    FFNN_label ->  label_token_sum

    label_token_sum ->  Loss_intent
    tmp3_cls -> Loss_intent

    // 结束 ：意图识别损失

    // 开始：CRF


        crf [label="CRF", style="filled", fillcolor="moccasin", fontcolor="#000000",  shape="box", width="5" ];
        entity_o [label="Entity:\n O", shape="ellipse", style="filled,rounded",fillcolor=antiquewhite,]
        entity_game1 [label="Entity:\n game name", shape="ellipse", style="filled,rounded",fillcolor=antiquewhite,]
        entity_game2 [label="Entity:\n game name", shape="ellipse", style="filled,rounded",fillcolor=antiquewhite,]

        crf -> {entity_o,entity_game1,entity_game2} [dir=back];

        {rank=same; entity_o->entity_game1->entity_game2[ style = invis ] }

        {tmp3_play,tmp3_ping,tmp3_mask,tmp3_cls} -> crf



    Loss_entity  [label="Entity Loss", style="filled", fillcolor="tomato", shape="box"]

    // 边的开始位置在结点右侧
    // 方位是按照 north east west south
    crf:e -> Loss_entity

    // 结束 ：CRF


    // {rank=same;entity_o,entity_game1,entity_game2,Loss_intent,Loss_lm_mask}
    {rank=same;Loss_entity,Loss_intent,Loss_lm_mask}

    Loss_total  [label="Total Loss", style="filled", fillcolor="tomato", shape="box"]

    {Loss_entity,Loss_intent,Loss_lm_mask} -> Loss_total



论文中的原图

.. figure:: https://pic4.zhimg.com/80/v2-61111bc8dd3d54aeb12616241cc1b95f_720w.jpg


相似度损失单元
===========================

.. digraph:: Similarity_Loss

    graph [rankdir=TB, clusterrank="local",fontsize=20];

        label="Similarity Loss";
        style="filled,rounded";

        input_1 [label="input1", shape="box",style="filled",]
        input_2 [label="input2", shape="box",style="filled",]

        embeding_1 [label="embedding", shape="box",style="filled",]
        embeding_2 [label="embedding", shape="box",style="filled",]
        similarity [label="similarity", shape="box",style="filled",]
        loss [label="Intent Loss", shape="box",style="filled",]

        input_1 -> embeding_1
        input_2 -> embeding_2
        {embeding_1,embeding_2} -> similarity
        similarity -> loss








DIET::batch_to_model_data_format


rasa 是一个多轮对话框架，这里的模型也是按照多轮进行设计的，
一条输入样本，包含对话(dialogue)中的多条消息(文本)，
每条文本由分成token序列。


其它学习资料
########################################

讲解视频
https://www.youtube.com/playlist?list=PL75e0qA87dlG-za8eLI6t0_Pbxafk-cxb


https://zhuanlan.zhihu.com/p/337181983