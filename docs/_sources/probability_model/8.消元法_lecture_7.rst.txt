########################################################
模型推断：消元法
########################################################
整个概率图模型的理论可以分为三大块：模型表示（Presentation）、
模型推断（Inference）和模型学习（Learning）。
我们已经讨论完模型的表示问题，
本章开始我们讨论模型的推断问题。
图模型中的概率推断问题一般有三类，分别是

- 边缘概率（Marginal probabilities）查询。
- 条件概率（Conditional probabilities）查询。
- 最大后验概率（Maximum a posterior probabilities,MAP）查询。

其中边缘概率查询和条件概率查询其实是等价的，在计算过程中没有太大区别，
因此通常会把这两类问题合并在一起。
在概率图模型中解决概率查询问题的算法有很多，
根据是否可以得到精确结果可以划分为两大类，
精确推断算法和近似推断算法，
顾名思义，精确推断算法可以得到准确的查询概率值，
而近似推断算法只能得到近似的结果。
既然已经有精确推断算法了，为什么还要近似算法呢？
显然，精确算法有它的局限性，不能解决所有的问题，
所以才需要通过损失精度的方法去解决所面临问题。
本章我们讨论图模型中可以精确进行概率查询的消元算法，
消元法是图模型中进行概率查询的基础算法，属于入门必须课。


.. digraph:: 模型推断
    :align: center
    :caption: 模型推断的类型和求解算法

    graph [rankdir=LR,]
    node [shape=box]

    n1 [label="概率推断"]

    n2 [label="推断问题"]
    n21 [label="边缘概率"]
    n22 [label="条件概率"]
    n23 [label="最大后验概率"]


    n3 [label="推断算法"]

    n31 [label="精确推断（Exact influence）"]
    n311 [label="消元法（Elimination algorithm）"]
    n312 [label="置信传播（Belief propagation algorithm）"]
    n313 [label="联结树算法（Junction Tree algorithm）"]

    n32 [label="近似推断（Approximate inference）"]
    n321 [label="采样法（Sampling algorithms）"]
    n322 [label="变分法（Variational methods）"]


    n1 -> {n2 n3}
    n2-> {n21 n22 n23}

    n3 ->{n31 n32}

    n31->{n311 n312 n313}
    n32->{n321 n322 }







.. math::

    \newcommand{\indep}{\perp  \!  \!\! \perp}
    \newcommand{\notindep}{\not\! \perp  \!  \!\! \perp}

    \mathcal{X} \indep \mathcal{Y} \mid \mathcal{Z}

    \mathcal{X} \notindep \mathcal{Y} \mid \mathcal{Z}



    X \independent Y

    \newcommand{\nb}[1]{\text{ne}(#1)}
    \newcommand{\pa}[1]{\text{pa}(#1)}
    \newcommand{\ch}[1]{\text{ch}(#1)}
    \newcommand{\de}[1]{\text{de}(#1)}
    \newcommand{\an}[1]{\text{an}(#1)}
    \newcommand{\nd}[1]{\text{nd}(#1)}
    \newcommand{\can}[1]{\overline{\text{an}}(#1)}
    \newcommand{\ind}{⫫} \newcommand{\dsep}{\perp_d}
    \newcommand{\msep}{\perp_m}

    X \ind Y


什么是模型的推断
########################################################

在前面的章节中，我们讨论了概率图模型的表示以及独立性的表达，
从中了解到概率图是对多个随机变量的联合概率分布的图形化表示。
实际应用中，我们经常需要基于图模型，计算其中部分结点变量的边缘概率或者条件概率。
比如，我们用E、F和R分别表示图中没有交集的结点集合，E、F和R组成了图模型中的全部结点，
:math:`X_E,X_F,X_R`
为对应的变量集合，图模型的联合概率分布为 :math:`P(X_E,X_F,X_R)` 。
我们可能需要计算边缘概率 :math:`p(x_E,x_F)` 或者条件概率 :math:`p(x_F|x_E)` ，
亦或者最大后验概率(MAP) :math:`max_{X_F} p(x_F|x_E)` ，
这就是图模型的概率推断问题(probabilistic inference problem)。
本章我们先讨论模型推断中计算边缘概率和条件概率的问题，最大后验概率在后续章节讨论。
在正式开始前，先通过一个例子直观的了解一下什么是模型推断。


.. topic:: 箱子里取球的概率问题

    假设我们有两个箱子分别为 :math:`a_1,a_2` ，箱子中分别装有红色球和白色球。假设 :math:`a_1` 箱子中有4个红色球和6个白色球，
    :math:`a_2` 箱子中有8个红色球和2个白色球。 另外我们有一个特殊的硬币，投放后正面向上的概率是0.6，反面向上的概率是0.4。
    现在我们进行如下实验：

    首先投掷硬币，然后观察硬币的朝向。
    如果正面向上就从 :math:`a_1` 箱子随机取出一个球，并记录球的颜色；
    如果反面朝上，就从 :math:`a_2` 箱子中随机取出一个球，并记录下球的颜色。

    我们设硬币朝向(或者说是决定的箱子)为随机变量A，则有 :math:`p(A=\text{正}(a_1))=0.6;p(A=\text{反}(a_2))=0.4` 。球的颜色记为随机变量B，
    则有 :math:`P(B=\text{红}|A=a_1)=0.4,\ P(B=\text{白}|A=a_1)=0.6 `,
    :math:`P(B=\text{红}|A=a_2)=0.8,\ P(B=\text{白}|A=a_2)=0.2`。


这个例子中有两个随机变量，分别是A和B，其联合概率分布为:

.. math::
    P(A,B) = P(A)P(B|A)

根据前几章的知识，可以用一个有向图表示这个联合概率分布。

.. digraph:: AB
    :align: center
    :caption: 随机变量A和B的有向图表示
    :alt: alternate text


    graph [rankdir=LR,]
    node [shape=circle]

    A -> B




假设有个人进行了一次实验，但是只告诉我们取到的是红色球，让我们去猜测他是从哪个箱子取出来的。
这时我们就需要根据观测变量B(球的颜色)去推断出未知的变量A（箱子），
也就是计算条件概率分布 :math:`P(A|B=\text{红色})` ，
这就是典型的模型推断问题，已知图中部分变量的值，去推断未知变量的概率分布。

.. digraph:: AB2
    :align: center
    :caption: 变量B是观测变量
    :alt: alternate text


    graph [rankdir=LR,]
    node [shape=circle]
    B[style=filled]

    A -> B



模型的推断都是建立在贝叶斯推断（定理）的基础上的，根据贝叶斯定理有:

    .. math::

        P(A|B=\text{观测值}) &= \frac{P(B=\text{观测值}|A) P(A)}{P(B=\text{观测值})}

        \text{后验概率分布}  &= \frac{\text{似然} \times \text{先验概率分布}}{\text{边缘概率分布}}

.. hint::

    如果有多次观测（样本），则其中的似然部分就是连乘式(全部观测值同时发生):

    .. math::

            P(A|B=\text{\{观测值集合\}}) = \frac{ \left [ \prod_i^N P_i(B=\text{观测值}_i|A) \right ] P(A)}{P(B=\text{\{观测值集合\}})}



    通常根据推断的结果不同，贝叶斯推断一般分为两类：

    - **计算后验概率分布。**  推断出的是未知变量的概率分布，而不是具体取值。

    - **计算最大后验估计。** 直接推断出未知变量的最大可能的取值。

        我们需要计算一个最大可能(最大概率的)的  :math:`A` 取值， :math:`\hat{a} = \mathop{\arg\max}_{A} P(A|B=b)` 。


有时我们会把图中观测变量集合记为证据(Evidence)变量集合 :math:`\mathbf{E}` ，
观测值集合称为证据 :math:`\mathbf{e}` ，
推断的变量集合称为 **查询变量** ，记为 :math:`\mathbf{X}` ，则有:

.. math::

    P(\mathbf{X}|\mathbf{E}=\mathbf{e}) = \frac{P(\mathbf{X,e})}{P(\mathbf{e})}



然而，通常情况我们的模型是比较复杂的，模型中并不是只有观测变量和推断变量，除了这两类变量外，
还存在其他的变量，既不是证据变量，也不是查询变量，我们称之为 **隐变量** ，
用 :math:`\mathbf{W}=\mathcal{X}-\mathbf{X}-\mathbf{E}` 表示图中隐变量的集合，
此时联合概率分布变为 :math:`P(\mathbf{X,W,e})` ，
这时我就就需要边际化的方法得到:

.. math::

    P(\mathbf{X,e}) = \sum_{\mathbf{W}} P(\mathbf{X,W,e})


概率 :math:`p(\mathbf{e})` 可以通过对联合概率分布求和来直接计算，不过也可以在上式的基础上，通过下式计算：

.. math::

    P(\mathbf{e}) =  \sum_{\mathbf{X}} P(\mathbf{X,e})

这种方法可以重复利用公式(8.5)的计算结果。



.. 我们关注于计算无向图模型的后验概率分布。具体的，考虑一个包含有一系列随机变量 :math:`x_1,\dots,x_N` 的无向图 :math:`G` ,其因子式为：
    .. math::

        p_{\mathbf{x}}(\mathbf{X}) = \frac{1}{Z} \prod_{c \in C } \varphi(x_c)
    其中，C是图中最大团的集合，Z是配分函数（归一化系数），:math:`\varphi(x_c)` 是定义在最大团c上的势函数 。
    假设 :math:`x_N` 是观测变量，我们想要计算在观测到 :math:`x_N` 时， 变量 :math:`x_1` 的后验概率分布 :math:`p_{x_1|x_N}(\centerdot|x_N)` ，
    这时就需要对 :math:`x_2,\dots,x_{N-1}` 进行消除达到边际化(marginalize) :math:`x_1`， 更一般的有:
    .. math::

        p_{x_A|x_B}(X_A|X_B) = \frac{p_{x_A,x_B}(X_A,X_B)}{p_{x_B}(X_B)}

    对于任意不相交子集 :math:`A,B \subset \{1,2,\dots,N\}, A \cap B = \varnothing` ，其中 :math:`A \cup B` 可能不包含G中所有节点。



总结下，条件概率的推断过程是：

1. 根据贝叶斯定理列出查询变量 :math:`\mathbf{X}` 的条件概率。

.. math::
    P(\mathbf{X}|\mathbf{E}=\mathbf{e}) = \frac{P(\mathbf{X,e})}{P(\mathbf{e})}


2. 计算分子。通过边际化(marginalization)的方法，消除概率图中联合概率分布 :math:`P(\mathbf{X,W,e})` 中的隐变量 :math:`\mathbf{W}` ，得到观测变量和查询变量的联合概率分布 :math:`P(\mathbf{X,e})` 。


    .. math::

        P(\mathbf{X,e}) = \sum_{\mathbf{W}} P(\mathbf{X,W,e})

3. 计算分母。边际化分子求得。

    .. math::
            P(\mathbf{e}) =  \sum_{\mathbf{X}} P(\mathbf{X,e})


显然，在推断过程中一个很重要的工作就是对联合概率分布进行边际化，以求的部分变量子集的边缘概率分布。
所以概率模型推断的核心就是边际化算法。而边际化最直接的算法就是 **消元法** 。
这个算法适用于所有无向图，并且提供了一种精确推断的方法，唯一的不足是可能具有很高的计算复杂度。
消元法也可以应用于有向无环图(DAG)，比如，我们可以把DAG转化为无向图然后再应用消元法。


在进行变量消除(消元)处理时，离散变量概率分布求和，连续值变量概率密度函数求积分；
而观测变量就把求和(积分)操作替换成取观测值时的概率值就行。

消元法
########################################################

模型推断的算法有很多种，整体上可以分为两大类：精确推断和近似推断。
精确推断算法，可以得出准确的结果，常见的有消元法、置信传播算法等等。
近似推断算法，顾名思义，只能得到一个近似的结果，常见的有变分法、采样法等。
其中，消元法是最基础的方法，算法过程非常直观，并且兼容性很好，
在有向图和无向图中都可以使用，
并且整个过程是等价的。
然而消元法也有一些限制和不足，最典型的就是计算复杂度较高，
但它非常适合作为入门算法。

一个例子
============================


.. _fg_7_a1:

.. figure:: pictures/7_a1.jpg
    :scale: 40 %
    :align: center

    深色阴影结点 :math:`\mathrm{x}_6` 是条件变量(证据变量)，浅色阴影结点是需要消除的变量，我们想要计算条件概率 :math:`p(x_1|x_6)` 。
    :math:`E=\{6\},F=\{1\},R=\{2,3,4,5\}`


.. _fg_7_a2:

.. digraph:: example1
    :align: center
    :caption: 深色阴影结点 :math:`X_6` 是条件变量(证据变量)，浅色阴影结点是需要消除的变量（隐变量）。
    :alt: alternate text

    graph [rankdir=LR,]
    node [shape=circle]

    X1[label=<X<SUB>1</SUB>>]
    X2[style=filled, label=<X<SUB>2</SUB>>]
    X3[style=filled, label=<X<SUB>3</SUB>> ]
    X4[style=filled, label=<X<SUB>4</SUB>>]
    X5[style=filled, label=<X<SUB>5</SUB>>]
    X6[style=filled, label=<X<SUB>6</SUB>>, fillcolor=dimgrey]


    X1 -> {X2 X3}
    X2 -> {X4 X6}
    X3 -> X5
    X5 -> X6





如 :numref:`fg_7_a2` 所示，我们符号 :math:`V` 表示图中全部结点的集合，:math:`V=\{X_1,X_2,X_3,X_4,X_5,X_6\}`。
其中深度阴影(深色)表示可以观测的结点，有时也称为 *证据结点(evidence nodes)* ，
用 :math:`E=\{X_6\}` 表示。
轻度阴影(浅色)结点表示隐变量集合，即无法进行观测的结点，
用符号 :math:`W=\{ X_2,X_3,X_4,X_5\}` 表示。
其中非阴影结点 :math:`X_1` 为 *查询结点(query nodes)* ，记为 :math:`F=\{X_1\}` 。
我们要计算在已有证据结点观测值的条件下，查询结点的（边缘）概率分布。

此外，我们约定用大写字母 :math:`X` 表示随机变量，
对应的小写字母 :math:`x` 表示随机变量的一种可能取值(不固定)，
用花式符号 :math:`\mathcal{X}` 表示变量的取值空间，即 :math:`x \in \mathcal{X}` 。
头上有横线的符号 :math:`\bar{x}` 表示取值空间 :math:`\mathcal{X}` 中一个 *特定* 的值。


假设图中所有结点都是二值离散的伯努利随机变量，只有 :math:`0` 或 :math:`1` 两个可能的取值，
即 :math:`\mathcal{X}=\{0,1\}`。
由于所有结点都是离散变量，
图中所有的局部概率分布都可以用表格的形式给出。

.. math::

    P(X_{1}) =
    \begin{array}{|c|c|}
    \hline
    x_1&probability\\
    \hline
    0&0.3\\
    \hline
    1&0.7\\
    \hline
    \end{array}


.. math::

    P(X_{2}|X_1)=
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&probability\\
    \hline
    0&0&0.4\\
    0&1&0.6\\
    \hline
    1&0&0.5\\
    1&1&0.5\\
    \hline
    \end{array}

.. math::

    P(X_{3}|X_1)=
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_3&probability\\
    \hline
    0&0&0.3\\
    0&1&0.7\\
    \hline
    1&0&0.6\\
    1&1&0.4\\
    \hline
    \end{array}


.. math::


    P(X_{4}|X_2)=
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_4&probability\\
    \hline
    0&0&0.2\\
    0&1&0.8\\
    \hline
    1&0&0.7\\
    1&1&0.3\\
    \hline
    \end{array}


.. math::

    P(X_{5}|X_3)=
    \begin{array}{|c|c|c|}
    \hline
    x_3&x_5&probability\\
    \hline
    0&0&0.7\\
    0&1&0.3\\
    \hline
    1&0&0.3\\
    1&1&0.7\\
    \hline
    \end{array}

.. math::

    P(X_{6}|X_2,X_5)=
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_6&probability\\
    \hline
    0&0&0&0.4\\
    0&0&1&0.6\\
    \hline
    0&1&0&0.8\\
    0&1&1&0.2\\
    \hline
    1&0&0&0.1\\
    1&0&1&0.9\\
    \hline
    1&1&0&0.3\\
    1&1&1&0.7\\
    \hline
    \end{array}








在给定 :math:`X_E` 的观测值的条件下，计算出查询结点 :math:`X_F` 的条件概率分布 :math:`P(X_F|X_E=x_E)` 。
由于图中存在隐变量，
因此首先要消除隐变量，
得到查询变量和证据变量的边缘概率分布。

.. math::

    P(X_E,X_F) = \sum_{X_W} P(X_E,X_F,X_W)

然后在此基础上边际化得到 :math:`p(x_E)` ：

.. math::

    p(x_E) = \sum_{x_F} p(x_E,x_F)


最后得到条件概率 :math:`p(x_F|x_E)` ：

.. math::

    p(x_F|x_E) = \frac{p(x_E,x_F)}{p(x_E)}


这里我们假设要计算当 :math:`X_6=\bar{x}_6` 时， :math:`X_1=x_1` 的概率，
即条件概率 :math:`p(X_1=x_1|X_6=\bar{x}_6)` 。


第一步，计算边缘概率 :math:`p(X_1=x_1,X_6=\bar{x}_6)` 。



.. math::
    :label: eq_7_1

    p(x_1,\bar{x}_6) &= \sum_{x_2,x_3,x_4,x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= \sum_{x_2} \sum_{x_3} \sum_{x_4} \sum_{x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)



这里我们假设 :math:`\bar{x}_6=1` ，按照 :eq:`eq_7_1` 中的消元顺序，我们需要先计算 :math:`\sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)`
以消除变量 :math:`\mathrm{x}_5` 。我们把 :math:`\bar{x}_6=1` 代入到 :math:`p(\bar{x}_6|x_2,x_5)` 可得：

.. math::

    p(\bar{x}_6=1|x_2,x_5)=
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_6&probability\\
    \hline
    0&0&1&0.6\\
    \hline
    0&1&1&0.2\\
    \hline
    1&0&1&0.9\\
    \hline
    1&1&1&0.7\\
    \hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_5&probability\\
    \hline
    0&0&0.6\\
    \hline
    0&1&0.2\\
    \hline
    1&0&0.9\\
    \hline
    1&1&0.7\\
    \hline
    \end{array}

然后我们定义
:math:`m_5(x_2,x_3)\triangleq \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)` 。在后续章节我们会看到这一步其实会把
:math:`X_5` 结点从图中消除掉。注意，:math:`m_i` 函数只是表示消元后的结果信息，并没有概率意义，不需要符合概率的约束(和为1)。

.. math::

     p(\bar{x}_6=1|x_2,x_5) p(x_5|x_3)  &=
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_5&probability\\
    \hline
    0&0&0.6\\
    \hline
    0&1&0.2\\
    \hline
    1&0&0.9\\
    \hline
    1&1&0.7\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_3&x_5&probability\\
    \hline
    0&0&0.7\\
    0&1&0.3\\
    \hline
    1&0&0.3\\
    1&1&0.7\\
    \hline
    \end{array}

    &=
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_3&message\\\hline
    0&0&0& 0.6\times0.7=0.42 \\\hline
    0&0&1& 0.6\times0.3=0.18 \\\hline
    0&1&0& 0.2\times0.3=0.06 \\\hline
    0&1&1& 0.2\times0.7=0.14 \\\hline
    1&0&0& 0.9\times0.7=0.63 \\\hline
    1&0&1& 0.9\times0.3=0.27 \\\hline
    1&1&0& 0.7\times0.3=0.21 \\\hline
    1&1&1& 0.7\times0.7=0.49 \\\hline
    \end{array}


.. math::

    m_5(x_2,x_3) = \sum_{x_5} p(\bar{x}_6=1|x_2,x_5) p(x_5|x_3)=
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_3&message\\\hline
    0&0& 0.42+0.06=0.48\\\hline
    0&1& 0.18+0.14=0.32 \\\hline
    1&0& 0.63+0.21=0.84\\\hline
    1&1& 0.27+0.49=0.76 \\\hline
    \end{array}


把 :math:`m_5(x_2,x_3)` 代入到 :eq:`eq_7_1` 可得：

.. math::
    :label: eq_7_2

    p(x_1,\bar{x}_6)  &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) \sum_{x_4} p(x_4|x_2)



然后我们继续计算 :math:`\sum_{x_4} p(x_4|x_2)` 消除掉变量 :math:`\mathrm{x}_4` ，并且我们定义
:math:`m_4(x_2) = \sum_{x_4} p(x_4|x_2)` 。

.. math::

    m_4(x_2)=\sum_{x_4} p(x_4|x_2) = \sum_{x_4}
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_4&probability\\
    \hline
    0&0&0.2\\
    0&1&0.8\\
    \hline
    1&0&0.7\\
    1&1&0.3\\
    \hline
    \end{array}
    =
    \begin{array}{|c|c|}
    \hline
    x_2&message\\
    \hline
    0&1\\
    \hline
    1&1\\
    \hline
    \end{array}

将 :math:`m_4(x_2)` 代入到 :eq:`eq_7_2` ：

.. math::
    :label: eq_7_3

    p(x_1,\bar{x}_6) &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) m_4(x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)


类似的我们定义 :math:`m_3(x_1,x_2)=\sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)` ，计算可得：

.. math::

    p(x_3|x_1) m_5(x_2,x_3) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_3&probability\\
    \hline
    0&0&0.3\\
    0&1&0.7\\
    \hline
    1&0&0.6\\
    1&1&0.4\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_3&message\\
    \hline
    0&0&0.48\\
    \hline
    0&1&0.32\\\hline
    1&0&0.84\\\hline
    1&1&0.76\\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|c|}
    \hline
    x_1&x_2&x_3&message\\
    \hline
    0&0&0& 0.3\times 0.48=0.144 \\\hline
    0&0&1& 0.7\times 0.32=0.224 \\\hline
    0&1&0& 0.3\times 0.84=0.252 \\\hline
    0&1&1& 0.7\times 0.76=0.532 \\\hline
    1&0&0& 0.6\times 0.48=0.288 \\\hline
    1&0&1& 0.4\times 0.32=0.128\\\hline
    1&1&0& 0.6\times 0.84=0.504\\\hline
    1&1&1& 0.4\times 0.76=0.304\\\hline
    \end{array}

.. math::
    m_3(x_1,x_2) = \sum_{x_3}  p(x_3|x_1) m_5(x_2,x_3) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.144+0.224=0.368\\\hline
    0&1& 0.252+0.532=0.784\\\hline
    1&0& 0.288+0.128=0.416 \\\hline
    1&1& 0.504+0.304=0.808 \\\hline
    \end{array}



将 :math:`m_3(x_1,x_2)` 代入 :eq:`eq_7_3` 可得：

.. math::
    :label: eq_7_4

    p(x_1,\bar{x}_6) =p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)


同样的我们定义 :math:`m_2(x_1)=\sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)` ，计算可得：


.. math::
    m_4(x_2) m_3(x_1,x_2) =
    \begin{array}{|c|c|}
    \hline
    x_2&message\\
    \hline
    0&1\\
    \hline
    1&1\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}


.. math::

    p(x_2|x_1) [m_4(x_2) m_3(x_1,x_2)] =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&probability\\
    \hline
    0&0&0.4\\
    0&1&0.6\\
    \hline
    1&0&0.5\\
    1&1&0.5\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.4\times 0.368=0.1472 \\\hline
    0&1& 0.6\times 0.784=0.4704\\\hline
    1&0& 0.5\times 0.416=0.208 \\\hline
    1&1& 0.5\times 0.808=0.404 \\\hline
    \end{array}


.. math::

    m_2(x_1)=\sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.1472+0.4704=0.6176\\\hline
    1& 0.208 + 0.404=0.612 \\\hline
    \end{array}


代入到 :eq:`eq_7_4` 可得：

.. math::
    :label: eq_7_5

    p(x_1,\bar{x}_6) =p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)
    = p(x_1)m_2(x_1)

    =
    \begin{array}{|c|c|}
    \hline
    x_1&probability\\
    \hline
    0&0.3\\
    \hline
    1&0.7\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.6176\\\hline
    1& 0.612 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.3\times0.6176 =0.18528  \\\hline
    1& 0.7\times0.612 =0.4284  \\\hline
    \end{array}



我们重新整理一下整个计算过程：

.. math::


    p(x_1,\bar{x}_6) &= \sum_{x_2,x_3,x_4,x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= \sum_{x_2} \sum_{x_3} \sum_{x_4} \sum_{x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) \sum_{x_4} p(x_4|x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)  m_4(x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)

    &=  p(x_1)m_2(x_1)



在这基础上，我们通过消除变量 :math:`\mathrm{x}_6` 计算出概率 :math:`p(\bar{x}_6)` ：

.. math::
    :label: eq_7_6

    p(\bar{x}_6=1) = \sum_{x_1}p(x_1)m_2(x_1) = 0.18528+0.4284=0.61368


然后根据贝叶斯定理，我们可以计算出条件概率 :math:`p(x_1|\bar{x}_6=1)` ：

.. math::

    p(x_1|\bar{x}_6=1) = \frac{p(x_1)m_2(x_1)}{\sum_{x_1}p(x_1)m_2(x_1)}
    =\frac{p(x_1)m_2(x_1)}{p(\bar{x}_6=1)}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.18528/0.61368 \approx 0.3 \\\hline
    1& 0.4284/0.61368  \approx 0.7 \\\hline
    \end{array}


我们发现 :eq:`eq_7_5` 中的边缘概率 :math:`p(x_1,\bar{x}_6)` 相当于是未归一化的条件概率 :math:`p(x_1|\bar{x}_6=1)` ，
通过 :eq:`eq_7_6` 的归一化常量 :math:`p(\bar{x}_6=1)` 可以计算出条件概率 :math:`p(x_1|\bar{x}_6)` 。

到此我们通过一个具体的例子演示了消元法的计算过程，消元法是概率图模型进行推断的直接算法，是一种精确的推断算法。
但是其有个明显的缺点就是，每一次概率查询(条件概率、边缘概率推断)都需要执行一次上述过程，比如，如果我们想要查询条件概率
:math:`p(x_1|\bar{x}_6=0),p(x_1|\bar{x}_4),\ldots` 等等，都需要分别执行一次上述过程，计算复杂度非常高。
然而，我们可以发现在执行不同的概率查询（对端）时，有些计算是重复的，
下一章节我们会讨论如何重复利用中间结果减少计算量的模型推断算法-和积算法(sum-product algorithm)。


.. _ch_condition_margin:

条件概率和边缘概率
===========================

为了能更直观的解释消元法，这里我们通过一些定义把条件变量取定值的操作也转化成求和操作，
通过这样的转化可以令条件变量和边际化消除变量具有相同的操作，更容易理解和操作。



我们令 :math:`\mathrm{x}_i` 表示证据变量，其观测值是 :math:`\bar{x}_i` 。
我们定义一个 *证据势函数(evidence potential)* ， :math:`\delta(x_i,\bar{x}_i)` ，
当 :math:`x_i=\bar{x}_i` 成立时这个函数值为1，否则为0。
通过这个函数我们可以把推断过程中对证据变量 :math:`\mathrm{x}_i` 的限制约束 (取值为 :math:`\bar{x}_i` ) 操作转化成求和操作。
函数 :math:`g(x_i)` 表示变量 :math:`\mathrm{x}_i` 的一个函数，比如在上面的例子中
:math:`g(x_6)=p(x_6|x_2,x_5)` ，通过下面的转换可以把 :math:`g(\bar{x}_6)=p(\bar{x}_6|x_2,x_5)` 转换成等价的求和操作。



.. math::

    g(\bar{x}_i)=\sum_{x_i} g(x_i) \delta(x_i,\bar{x}_i)
    = \sum_{x_i} p(x_6|x_2,x_5) \delta(x_i,\bar{x}_i)

这样在执行消元推断时，对于条件(证据)变量的限制约束操作转化为一个求和操作，二者的值是等价，
这样在上面的例子中就可以额外定义出 :math:`m_6(x_2,x_5)` 。

.. math::

    m_6(x_2,x_5) = \sum_{x_6} p(x_6|x_2,x_5) \delta(x_6,\bar{x}_6) = p(\bar{x}_6|x_2,x_5)




更一般的，我们可以扩展到多个条件变量的情形，我们用E表示条件变量集合，对于特定的条件(观测、证据)值
:math:`\bar{x}_E` ，我们想要计算 :math:`p(x_F|\bar{x}_E)` 。这时我们定义一个 *整体证据势函数(total evidence potential)* ：

.. math::

    \delta(x_E,\bar{x}_E) \triangleq \prod_{i\in E} \delta(x_i,\bar{x}_i)

只有当 :math:`x_E=\bar{x}_E` 成立时，这个函数为1，否则为0。通过这个势函数，我们可以把条件概率 :math:`p(x_F|\bar{x}_E)`
的分子分母都表示成求和的形式，分母其实就是分子的归一化，是在分子的基础上进行求和。

.. math::

    p(x_F,\bar{x}_E) = \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)

    p(\bar{x}_E) = \sum_{x_F} \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)

条件概率 :math:`p(x_F|\bar{x}_E)` 的计算公式为：

.. math::

    p(x_F|\bar{x}_E) &= \frac{p(x_F,\bar{x}_E)}{p(\bar{x}_E)}

    &= \frac{\sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)}{\sum_{x_F} \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)}


条件概率 :math:`p(x_F|\bar{x}_E)` 的分母 :math:`p(\bar{x}_E)` 是其分子 :math:`p(x_F,\bar{x}_E)` 的累加求和，
也就是说其实只要计算出分子部分，分母就自然得到了，从某种角度上讲只要计算出 :math:`p(x_F,\bar{x}_E)` 就相当于计算出
条件概率 :math:`p(x_F|\bar{x}_E)` ，那么我们可以把 :math:`p(x_F,\bar{x}_E)` 看成是条件概率令一种表示。
然而 :math:`p(x_F,\bar{x}_E)` 本身又是在边缘概率 :math:`p(x_F,x_E)` 的基础上加了一个证据势函数
:math:`p(x_F,\bar{x}_E)=p(x_F,x_E)\delta(x_E,\bar{x}_E)` 。我们可以用不加修改的消元法计算
:math:`p(x_F,x_E)` 和 :math:`p(x_F,\bar{x}_E)` ，本质上就是模型的条件概率和边缘概率的推断问题可以看成是等价的。
两者都是进行边际化消除，计算逻辑是一样的，不同的是 :math:`p(x_F,\bar{x}_E)` 多了一个证据势函数 :math:`\delta(x_E,\bar{x}_E)` 。

**通过引入证据势函数，我们把条件变量的值限定操作转换成了求和消除操作，条件变量可以和其他被边缘化消除的变量在操作上同等看待，**
**这样一来在图模型上进行条件概率查询也可以看做是进行边缘概率查询，因为两者在计算上是等价的。**
**也就是说我们可以把** :math:`p(x_F|\bar{x}_E)` 和 :math:`p(x_F,x_E)` **都当成是在求"边缘概率"，**
**在图模型的推断算法的讨论中，我们将不再区分两者，都会按照边缘概率查询来讨论。**
这样的方式同样适用于无向图，对于证据变量集合E，为其中每个结点的局部势函数 :math:`\psi_i(x_i)` 乘上 :math:`\delta(x_i,\bar{x}_i)` 。

.. math::
    \psi_i^E (x_i) \triangleq \psi_i(x_i)\delta(x_i,\bar{x}_i) ,i \in E




一个包含证据(观测值)值的有向图的条件概率，可以用的联合（边缘）概率的形式表示，其中E表示证据变量集合：

.. math::

    p^E(x) \triangleq p(x) \delta (x_E,\bar{x}_E)


对于无向图可以有同样的定义：

.. math::

    p^E(x) \triangleq \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi^E_{\mathrm{x}_C} (x_C)


有向图的消元法
===========================

在上一节的例子中我们已经展示了有向图的消元算法的过程，这里我们在重新整理一下有向图中的消元算法。

这个算法执行过程中的每一步都是在一个因子函数乘积上执行一个求和消元的过程，
这些因子函数可以是局部条件概率 :math:`p(x_i|x_{\pi_i})` 、证据势函数 :math:`\delta(x_i,\bar{x}_i)`
、中间信息因子 :math:`m_i(x_{S_i})` 。所有的这些函数都是定义在局部结点子集上的，这里统一用 "势函数(potential function)" 表示。
所以消元算法其实是一个在势函数的乘积上面通过求和消除变量的过程。

我们整理一下有向图中消元算法的伪代码过程：


    消元整体过程( :math:`\mathcal{G},E,F` )

        过程1： 初始化图和查询变量( :math:`\mathcal{G},F` )

        过程2：引入证据( :math:`E`  )

        过程3：更新( :math:`\mathcal{G}` )

        过程4：归一化查询变量(F)

    过程1. 初始化图和查询变量( :math:`\mathcal{G},F` )

        选择一个消元的顺序 :math:`I` ，F 变量排在最后。

        **foreach** :math:`\mathrm{x}_i` **in** :math:`\mathcal{V}` :

            把 :math:`p(x_i|x_{\pi_i})` 放到激活列表中  // *生成联合概率因子分解的过程*

        **end for**

    过程2. 引入证据( :math:`E`  )

        **foreach** i **in** E:
            把 :math:`\delta(x_i,\bar{x}_i)` 加入到激活列表中

        **end for**

    过程3：更新( :math:`\mathcal{G}` )

        **foreach** i **in** :math:`I` :

            从激活列表中找到所有包含 :math:`x_i` 的势函数从激活列表中去掉这下势函数

            令 :math:`\phi_i(x_{T_i})` 表示这些势函数的乘积

            令 :math:`m_i(x_{S_i})=\sum_{x_i} \phi_i(x_{T_i})`

            将 :math:`m_i(x_{S_i})` 加入到激活列表中

        **end for**

    过程4：归一化查询变量(F)

        :math:`p(x_F|\bar{x}_E) \leftarrow \frac{\phi_F(x_F)}{\sum_{x_F} \phi_F(x_F)}`



注意，在上述伪代码流程中我们定义了符号 :math:`T_i=\{i\}\cup S_i` ，表示的是求和子项 :math:`\sum_{x_i}`
中包含的全部结点子集。当消元过程执行到只剩下查询变量 :math:`\mathrm{x}_F` 时算法结束，这时我们就得到了未归一化的
"条件概率" :math:`p(x_F,\bar{x}_E)` ，通过在其上面对 :math:`x_F` 求和可以得到归一化因子 :math:`p(\bar{x}_E)` 。
让我们回到上面的例子中，按照伪代码的过程阐述一遍。

**过程1，初始化图和查询变量。**
首先我们确定证据结点 :math:`\mathrm{x}_6` ，查询结点是 :math:`\mathrm{x}_1` 。
选定消元顺序 :math:`I=(6,5,4,3,2,1)` ，其中查询结点排在最后面。
然后把所有局部条件概率放到激活列表中
:math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),p(x_6|x_2,x_5)\}` 。

**过程2. 引入证据。**
把证据势函数 :math:`\delta (x_6,\bar{x}_6)`
追加到激活列表中

.. math::

    \{ p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),p(x_6|x_2,x_5),\delta(x_6,\bar{x}_6) \}


**过程3：更新。**
    首先消除结点 :math:`\mathrm{x}_6` ，激活列表中包含变量 :math:`\mathrm{x}_6` 的"势函数(potential function)"有
    :math:`p(x_6|x_2,x_5)` 和 :math:`\delta(x_6,\bar{x}_6)` ，所以我们有
    :math:`\phi_6(x_2,x_5,x_6)=p(x_6|x_2,x_5)\delta(x_6,\bar{x}_6)` ，对 :math:`x_6` 进行求和得到
    :math:`m_6(x_2,x_5)=p(\bar{x}_6|x_2,x_5)` 。把这个新的势函数加入到激活列表中，并且从激活列表中移除
    :math:`p(x_6|x_2,x_5)` 和 :math:`\delta(x_6,\bar{x}_6)` 。至此，我们就完成了证据的引入，把
    :math:`p(x_6|x_2,x_5)` 限定为 :math:`\bar{x}_6` 。此时激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),m_6(x_2,x_5)\}`

    现在开始消除变量 :math:`\mathrm{x}_5` ，激活列表中包含变量 :math:`\mathrm{x}_5` 的势函数有
    :math:`p(x_5|x_3)` 和 :math:`m_6(x_2,x_5)` ，移除它们，然后定义
    :math:`\phi_5(x_2,x_3,x_5)=p(x_5|x_3)m_6(x_2,x_5)` ，对 :math:`\mathrm{x}_5` 进行求和得到
    :math:`m_5(x_2,x_3)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),m_5(x_2,x_3)\}` 。

    现在开始消除变量 :math:`\mathrm{x}_4` ，激活列表中包含变量 :math:`\mathrm{x}_4` 的势函数有
    :math:`p(x_4|x_2)` ，移除它，然后定义
    :math:`\phi_4(x_2,x_4)=p(x_4|x_2)` ，对 :math:`\mathrm{x}_4` 进行求和得到
    :math:`m_4(x_2)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),m_4(x_2),m_5(x_2,x_3)\}` 。


    现在开始消除变量 :math:`\mathrm{x}_3` ，激活列表中包含变量 :math:`\mathrm{x}_3` 的势函数有
    :math:`p(x_3|x_1)` 和 :math:`m_5(x_2,x_3)` ，移除它们，然后定义
    :math:`\phi_3(x_1,x_2,x_3)=p(x_3|x_1)m_5(x_2,x_3)` ，对 :math:`\mathrm{x}_3` 进行求和得到
    :math:`m_3(x_1,x_2)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)\}` 。


    现在开始消除变量 :math:`\mathrm{x}_2` ，激活列表中包含变量 :math:`\mathrm{x}_2` 的势函数有
    :math:`p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)` ，移除它们，然后定义
    :math:`\phi_2(x_1,x_2)=p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)` ，对 :math:`\mathrm{x}_2` 进行求和得到
    :math:`m_2(x_1)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),m_2(x_1)\}` 。

**过程4：归一化查询变量。**
    现在我们得到了 :math:`\phi_1(x_1)=p(x_1)m_2(x_1)` ，这其实就是"未归一化的条件概率" :math:`p(x_1,\bar{x}_6)` ，
    在其基础上消除 :math:`x_1` 得到 :math:`m_1=\sum_{x_1} \phi_1(x_1)` 就是归一化因子 :math:`p(\bar{x}_6)` 。




无向图的消元法
===========================

有向图的消元算法同样也适用于无向图，并不需要过多改变。唯一的变化就是激活列表中有向图的局部条件概率变成无向图的势函数
:math:`\{\psi_{\mathrm{x}_C}(x_C)\}`。
让我们考虑一个无向图的示例 ，这个无向图是上节的有向图转化而来。




.. _fg_7_a3:
.. figure:: pictures/7_a3.jpg
    :scale: 40 %
    :align: center

    无向图示例，深色阴影结点 :math:`\mathrm{x}_6` 是条件变量；浅色阴影结点，
    :math:`\{\mathrm{x}_2,\mathrm{x}_3,\mathrm{x}_4,\mathrm{x}_5\}` ，是需要边际化消除的变量集合；
    :math:`\mathrm{x}_1` 是查询结点。

如 :numref:`fg_7_a3` ，我们继续以查询条件概率 :math:`p(x_1|\bar{x}_6)` 为例，我们用定义在团上的势函数表示这个无向图的联合概率分布，
图上的团有 :math:`\{\mathrm{x_1},\mathrm{x_2}\},\{\mathrm{x_1},\mathrm{x_3}\},\{\mathrm{x_2},\mathrm{x_4}\},\{\mathrm{x_3},\mathrm{x_5}\},\{\mathrm{x_2},\mathrm{x_5},\mathrm{x_6}\}`
，则这个无向图的联合概率分布为：

.. math::

    p_{\mathbf{x}}(\mathbf{x}) =\frac{1}{Z} \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4) \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6)


类似于有向图的推断过程，首先我们计算未归一化的条件概率 :math:`p(x_1,\bar{x}_6)` 。

.. math::

    p(x_1,\bar{x}_6) &= \frac{1}{Z} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6) \delta(x_6,\bar{x}_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) \sum_{x_6} \varphi_{256}(x_2,x_5,x_6) \delta(x_6,\bar{x}_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) m_6(x_2,x_5)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) \sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) m_3(x_1,x_2)

     &= \frac{1}{Z} m_2(x_1)


对 :math:`x_1` 进行求和边际化可得到归一化因子：

.. math::

    p(\bar{x}_6) = \frac{1}{Z} \sum_{x_1} m2(x_1)

然后我们就能得到想要查询的条件概率 :math:`p(x_1|\bar{x}_6)` ：

.. math::

    p(x_1|\bar{x}_6) = \frac{m_2(x_1)}{\sum_{x_1} m_2(x_1)}


**我们发现无向图的归一化系数Z无需计算出来，在进行条件概率查询时可以消除掉。**
然而当我们需要计算边缘概率 :math:`p(x_i)` 时，这个系数Z就无法被消除了，需要被准确的计算出来。
但也不是很困难，**系数Z其实就是对联合概率分布中全部变量进行求和消除的结果** ，
我们在求边缘概率 :math:`p(x_i)` 时，已经在执行变量消除了，
消除了除变量 :math:`x_i` 以外的所有结点得到 :math:`m_i(x_i)` ，
这时有 :math:`Z=\sum_{x_i} m_i(x_i)` 。
具体举例说明下，还是这个无向图，假设想要查询边缘概率 :math:`p(x_1)` ，不是条件概率，没有证据变量，也就不需要引入 :math:`\delta(\cdot)`

.. math::

    p(x_1) &= \frac{1}{Z} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) \sum_{x_6} \varphi_{256}(x_2,x_5,x_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) m_6(x_2,x_5)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) \sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) m_3(x_1,x_2)

    &= \frac{1}{Z} m_2(x_1)

这时有：

.. math::
    Z= \sum_{x_1} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6) =\sum_{x_1} m_2(x_1)





无向图的消元算法和有向图本质上是一样的，**消元法的本质就是找到一个合适顺序进行边际化消除变量。**



图消除
########################################################

.. todo::
    待补充



总结
########################################################


- 概率图推断通常关注两个问题: a.边缘概率查询；b.条件概率查询，有时也称为后验概率。
- 消元法的本质就是找到一个合适的顺序进行变量的边际化(离散变量求和，连续变量积分)。
- 消元法的相比原始方法提高了计算效率。


