########################################################
消元法(The Elimination Algorithm)
########################################################

我们已经讨论完概率图模型的表示，本章我们讨论如何在图模型中计算条件概率、边缘概率--即模型的概率推断(probabilistic inference)问题。

什么是模型的推断
########################################################


我们已经了解到概率图是对多个随机变量的联合概率分布的图形化表示，
实际应用中，我们经常需要基于图模型，计算其中部分结点变量的边缘概率或者条件概率。
比如，我们用E、F和R表示图中两个没有交集的结点集合，E、F和R组成了图模型中的全部结点，
:math:`\mathrm{x}_E,\mathrm{x}_F,\mathrm{x}_R`
为对应的变量集合，图模型的联合概率分布为 :math:`p(\mathrm{x}_E,\mathrm{x}_F,\mathrm{x}_R)` 。
我们可能需要计算边缘概率 :math:`p(x_E,x_F)` 或者条件概率 :math:`p(x_F|x_E)` ，
亦或者最大后验概率(MAP) :math:`max_{x_F} p(x_F|x_E)` ，
这就是图模型的概率推断问题(probabilistic inference problem)。
本章我们先讨论模型推断中计算边缘概率和条件概率的问题，最大后验概率在后续章节讨论。

我们先用一个具体的实例阐述一下什么是模型推断，回到我们最开始用的箱子里取球的例子。

.. topic:: 箱子里取球的概率问题

    假设我们有两个箱子分别为 :math:`a_1,a_2` ，箱子中分别装有红色球和白色球。假设 :math:`a_1` 箱子中有4个红色球和6个白色球，
    :math:`a_2` 箱子中有8个红色球和2个白色球。 另外我们有一个特殊的硬币，投放后正面向上的概率是0.6，反面向上的概率是0.4。
    现在我们进行如下实验：

    首先，投掷硬币，然后观察硬币的朝向。
    如果正面向上就从 :math:`a_1` 箱子随机取出一个球，并记录球的颜色；
    如果反面朝上，就从 :math:`a_2` 箱子中随机取出一个球，并记录下球的颜色。

    我们设硬币朝向(或者说是决定的箱子)为随机变量A，则有 :math:`P(\text{正}(a_1))=0.6;P(\text{反}(a_2))=0.4` 。球的颜色记为随机变量B，
    则有 :math:`P(\text{红}|a_1)=0.4;P(\text{白}|a_1)=0.6;P(\text{红}|a_2)=0.8;P(\text{白}|a_2)=0.2`


这个例子中有两个随机变量，分别是A和B，其联合概率分布为:

.. math::
    P(A,B) = P(A)P(B|A)

转化成有向图表示为:


.. figure:: ./_static/20190222164747.png
    :scale: 40%
    :align: center

    随机变量A和B组成的有向图

假设有个人进行了一次实验，但是只告诉我们最后是红色球，并没有告诉我们硬币的朝向，也就是没有告知是从哪个箱子取出的。
这时我们就需要根据观测变量B(球的颜色，也就是观测到随机变量B的取值为红色)去推断出未知的变量A，
也就是计算条件概率 :math:`p(A=a|B=\text{红色})` ，
这就是典型的模型推断问题，已知图中部分变量的值，去推断未知变量的概率分布。


模型的推断都是建立在贝叶斯推断（定理）的基础上的，根据贝叶斯定理有:

    .. math::

        P(A|B=\text{观测值}) &= \frac{P(B=\text{观测值}|A) P(A)}{P(B)}

        \text{后验概率分布}  &= \frac{\text{似然} \times \text{先验概率分布}}{\text{边缘概率分布}}

.. hint::

    如果有多个观测值，则其中的似然部分就是连乘式(全部观测值同时发生):

    .. math::

            P(A|B=\text{\{观测值集合\}}) = \frac{ [\prod_i^N P_i(B=\text{观测值}_i|A)] P(A)}{P(B)}



    通常根据推断的结果不同，贝叶斯推断一般分为两类：

    - **计算后验概率分布。**  推断出的是未知变量的概率分布，而不是具体取值。

    - **计算最大后验估计。** 直接推断出未知变量的最大可能的取值。

        我们需要计算一个最大可能(最大概率的)的  :math:`\mathbf{x}` 取值， :math:`\hat{x} = \mathop{\arg\min}_{x} P(x|y)` 。


有时我们会把观测变量记为证据(Evidence)变量 :math:`\mathbf{E}` ，观测值集合称为证据 :math:`\mathbf{e}` ，
推断的变量称为 **查询变量** ，记为 :math:`\mathbf{X}` ，则有:

.. math::

    P(\mathbf{X}|\mathbf{E}=\mathbf{e}) = \frac{P(\mathbf{X,e})}{P(\mathbf{e})}



然而，通常情况我们的模型是比较复杂的，模型中并不是只有观测变量和推断变量，除了这两类变量外，
还存在其他的变量，既不是证据变量，也不是查询变量，我们称之为 **隐变量** ，
用 :math:`\mathbf{W}=\mathcal{X}-\mathbf{X}-\mathbf{E}` 表示，
此时联合概率分布变为 :math:`P(\mathbf{X,W,e})` ，
这时我就就需要边际化的方法得到:

.. math::

    P(\mathbf{X,e}) = \sum_{\mathbf{W}} P(\mathbf{X,W,e})


概率 :math:`p(\mathbf{e})` 可以通过对联合概率分布求和来直接计算，不过也可以在上式的基础上，通过下式计算：

.. math::

    P(\mathbf{e}) =  \sum_{\mathbf{X}} P(\mathbf{X,e})

这种方法可以重复利用公式(8.5)的计算结果。



.. 我们关注于计算无向图模型的后验概率分布。具体的，考虑一个包含有一系列随机变量 :math:`x_1,\dots,x_N` 的无向图 :math:`G` ,其因子式为：
    .. math::

        p_{\mathbf{x}}(\mathbf{X}) = \frac{1}{Z} \prod_{c \in C } \varphi(x_c)
    其中，C是图中最大团的集合，Z是配分函数（归一化系数），:math:`\varphi(x_c)` 是定义在最大团c上的势函数 。
    假设 :math:`x_N` 是观测变量，我们想要计算在观测到 :math:`x_N` 时， 变量 :math:`x_1` 的后验概率分布 :math:`p_{x_1|x_N}(\centerdot|x_N)` ，
    这时就需要对 :math:`x_2,\dots,x_{N-1}` 进行消除达到边际化(marginalize) :math:`x_1`， 更一般的有:
    .. math::

        p_{x_A|x_B}(X_A|X_B) = \frac{p_{x_A,x_B}(X_A,X_B)}{p_{x_B}(X_B)}

    对于任意不相交子集 :math:`A,B \subset \{1,2,\dots,N\}, A \cap B = \varnothing` ，其中 :math:`A \cup B` 可能不包含G中所有节点。



总结下，条件概率的推断过程是：

1. 根据贝叶斯定理列出查询变量 :math:`\mathbf{X}` 的条件概率。

.. math::
    P(\mathbf{X}|\mathbf{E}=\mathbf{e}) = \frac{P(\mathbf{X,e})}{P(\mathbf{e})}


2. 计算分子。通过边际化(marginalization)的方法，消除概率图中联合概率分布 :math:`P(\mathbf{X,W,e})` 中的隐变量 :math:`\mathbf{W}` ，得到观测变量和查询变量的联合概率分布 :math:`P(\mathbf{X,e})` 。


    .. math::

        P(\mathbf{X,e}) = \sum_{\mathbf{W}} P(\mathbf{X,W,e})

3. 计算分母。边际化分子求得。

    .. math::
            P(\mathbf{e}) =  \sum_{\mathbf{X}} P(\mathbf{X,e})


显然，在推断过程中一个很重要的工作就是对联合概率分布进行边际化，以求的部分变量子集的边缘概率分布。
所以概率模型推断的核心就是边际化算法。而边际化最直接的算法就是 **消元法** 。
这个算法适用于所有无向图，并且提供了一种精确推断的方法，唯一的不足是可能具有很高的计算复杂度。
消元法也可以应用于有向无环图(DAG)，比如，我们可以把DAG转化为无向图然后再应用消元法。


在进行变量消除(消元)处理时，离散变量概率分布求和，连续值变量概率密度函数求积分；
而观测变量就把求和(积分)操作替换成取观测值时的概率值就行。

消元法
########################################################

一个例子
============================


.. _fg_7_a1:

.. figure:: pictures/7_a1.jpg
    :scale: 40 %
    :align: center

    深色阴影结点 :math:`\mathrm{x}_6` 是条件变量(证据变量)，浅色阴影结点是需要消除的变量，我们想要计算条件概率 :math:`p(x_1|x_6)` 。
    :math:`E=\{6\},F=\{1\},R=\{2,3,4,5\}`

如 :numref:`fg_7_a1` 所示，我们用深度阴影(深色)的结点表示条件变量，通常我们也会称这些结点为 *证据结点(evidence nodes)* ，
我们用E表示。图中的非阴影结点是我们要计算条件概率的结点，称之为 *查询结点(query nodes)* ，用符号F表示。
剩余的轻度阴影(浅色)结点，是需要边际化消除的结点，用符号 :math:`R=V\backslash(E\cup F)` 表示。
我们想要计算条件概率 :math:`p(x_F|x_E)` 。

根据上节提到的贝叶斯定理，需要先计算边缘概率：

.. math::

    p(x_E,x_F) = \sum_{x_R} p(x_E,x_F,x_R)

然后在此基础上边际化得到 :math:`p(x_E)` ：

.. math::

    p(x_E) = \sum_{x_F} p(x_E,x_F)


最后得到条件概率 :math:`p(x_F|x_E)` ：

.. math::

    p(x_F|x_E) = \frac{p(x_E,x_F)}{p(x_E)}



这里我们假设要计算当 :math:`\mathrm{x}_6=\bar{x}_6` 时， :math:`\mathrm{x}_1=x_1` 的概率，
即条件概率 :math:`p(x_1|\bar{x}_6)` 。一般我们用符号 :math:`\mathrm{x}` 表示随机变量；
符号 :math:`\mathcal{X}` 表示变量的取值空间；
小写符号 :math:`x` 表示变量的一种可能取值(不固定) :math:`x \in \mathcal{X}` ；
头上有横线的符号 :math:`\bar{x}` 表示 *固定* 的一个取值。
首先第一步，我们需要计算边缘概率 :math:`p(x_1,\bar{x}_6)` 。



.. math::
    :label: eq_7_1

    p(x_1,\bar{x}_6) &= \sum_{x_2,x_3,x_4,x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= \sum_{x_2} \sum_{x_3} \sum_{x_4} \sum_{x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

这里我们通过一个具体的实例，为大家演示一下计算过程。为了简单我们假设图中每个变量都是伯努利变量，
并且我们给出这个联合概率分布中的每个局部条件分布。


.. math::

    p(x_{1}) =
    \begin{array}{|c|c|}
    \hline
    x_1&probability\\
    \hline
    0&0.3\\
    \hline
    1&0.7\\
    \hline
    \end{array}


.. math::

    p(x_{2}|x_1)=
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&probability\\
    \hline
    0&0&0.4\\
    0&1&0.6\\
    \hline
    1&0&0.5\\
    1&1&0.5\\
    \hline
    \end{array}

.. math::

    p(x_{3}|x_1)=
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_3&probability\\
    \hline
    0&0&0.3\\
    0&1&0.7\\
    \hline
    1&0&0.6\\
    1&1&0.4\\
    \hline
    \end{array}


.. math::


    p(x_{4}|x_2)=
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_4&probability\\
    \hline
    0&0&0.2\\
    0&1&0.8\\
    \hline
    1&0&0.7\\
    1&1&0.3\\
    \hline
    \end{array}


.. math::

    p(x_{5}|x_3)=
    \begin{array}{|c|c|c|}
    \hline
    x_3&x_5&probability\\
    \hline
    0&0&0.7\\
    0&1&0.3\\
    \hline
    1&0&0.3\\
    1&1&0.7\\
    \hline
    \end{array}

.. math::

    p(x_{6}|x_3,x_5)=
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_6&probability\\
    \hline
    0&0&0&0.4\\
    0&0&1&0.6\\
    \hline
    0&1&0&0.8\\
    0&1&1&0.2\\
    \hline
    1&0&0&0.1\\
    1&0&1&0.9\\
    \hline
    1&1&0&0.3\\
    1&1&1&0.7\\
    \hline
    \end{array}


这里我们假设 :math:`\bar{x}_6=1` ，按照 :eq:`eq_7_1` 中的消元顺序，我们需要先计算 :math:`\sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)`
以消除变量 :math:`\mathrm{x}_5` 。我们把 :math:`\bar{x}_6=1` 代入到 :math:`p(\bar{x}_6|x_2,x_5)` 可得：

.. math::

    p(\bar{x}_6=1|x_2,x_5)=
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_6&probability\\
    \hline
    0&0&1&0.6\\
    \hline
    0&1&1&0.2\\
    \hline
    1&0&1&0.9\\
    \hline
    1&1&1&0.7\\
    \hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_5&probability\\
    \hline
    0&0&0.6\\
    \hline
    0&1&0.2\\
    \hline
    1&0&0.9\\
    \hline
    1&1&0.7\\
    \hline
    \end{array}

然后我们定义
:math:`m_5(x_2,x_3)\triangleq \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)` 。在后续章节我们会看到这一步其实会把
:math:`X_5` 结点从图中消除掉。注意，:math:`m_i` 函数只是表示消元后的结果信息，并没有概率意义，不需要符合概率的约束(和为1)。

.. math::

     p(\bar{x}_6=1|x_2,x_5) p(x_5|x_3)  =
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_5&probability\\
    \hline
    0&0&0.6\\
    \hline
    0&1&0.2\\
    \hline
    1&0&0.9\\
    \hline
    1&1&0.7\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_3&x_5&probability\\
    \hline
    0&0&0.7\\
    0&1&0.3\\
    \hline
    1&0&0.3\\
    1&1&0.7\\
    \hline
    \end{array}
    =
    \begin{array}{|c|c|c|c|}
    \hline
    x_2&x_5&x_3&message\\\hline
    0&0&0& 0.6\times0.7=0.42 \\\hline
    0&0&1& 0.6\times0.3=0.18 \\\hline
    0&1&0& 0.2\times0.3=0.06 \\\hline
    0&1&1& 0.2\times0.7=0.14 \\\hline
    1&0&0& 0.9\times0.7=0.63 \\\hline
    1&0&1& 0.9\times0.3=0.27 \\\hline
    1&1&0& 0.7\times0.3=0.21 \\\hline
    1&1&1& 0.7\times0.7=0.49 \\\hline
    \end{array}


.. math::

    m_5(x_2,x_3) = \sum_{x_5} p(\bar{x}_6=1|x_2,x_5) p(x_5|x_3)=
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_3&message\\\hline
    0&0& 0.42+0.06=0.48\\\hline
    0&1& 0.18+0.14=0.32 \\\hline
    1&0& 0.63+0.21=0.84\\\hline
    1&1& 0.27+0.49=0.76 \\\hline
    \end{array}


把 :math:`m_5(x_2,x_3)` 代入到 :eq:`eq_7_1` 可得：

.. math::
    :label: eq_7_2

    p(x_1,\bar{x}_6)  &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) \sum_{x_4} p(x_4|x_2)



然后我们继续计算 :math:`\sum_{x_4} p(x_4|x_2)` 消除掉变量 :math:`\mathrm{x}_4` ，并且我们定义
:math:`m_4(x_2) = \sum_{x_4} p(x_4|x_2)` 。

.. math::

    m_4(x_2)=\sum_{x_4} p(x_4|x_2) = \sum_{x_4}
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_4&probability\\
    \hline
    0&0&0.2\\
    0&1&0.8\\
    \hline
    1&0&0.7\\
    1&1&0.3\\
    \hline
    \end{array}
    =
    \begin{array}{|c|c|}
    \hline
    x_2&message\\
    \hline
    0&1\\
    \hline
    1&1\\
    \hline
    \end{array}

将 :math:`m_4(x_2)` 代入到 :eq:`eq_7_2` ：

.. math::
    :label: eq_7_3

    p(x_1,\bar{x}_6) &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) m_4(x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)


类似的我们定义 :math:`m_3(x_1,x_2)=\sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)` ，计算可得：

.. math::

    p(x_3|x_1) m_5(x_2,x_3) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_3&probability\\
    \hline
    0&0&0.3\\
    0&1&0.7\\
    \hline
    1&0&0.6\\
    1&1&0.4\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_2&x_3&message\\
    \hline
    0&0&0.48\\
    \hline
    0&1&0.32\\\hline
    1&0&0.84\\\hline
    1&1&0.76\\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|c|}
    \hline
    x_1&x_2&x_3&message\\
    \hline
    0&0&0& 0.3\times 0.48=0.144 \\\hline
    0&0&1& 0.7\times 0.32=0.224 \\\hline
    0&1&0& 0.3\times 0.84=0.252 \\\hline
    0&1&1& 0.7\times 0.76=0.532 \\\hline
    1&0&0& 0.6\times 0.48=0.288 \\\hline
    1&0&1& 0.4\times 0.32=0.128\\\hline
    1&1&0& 0.6\times 0.84=0.504\\\hline
    1&1&1& 0.4\times 0.76=0.304\\\hline
    \end{array}

.. math::
    m_3(x_1,x_2) = \sum_{x_3}  p(x_3|x_1) m_5(x_2,x_3) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.144+0.224=0.368\\\hline
    0&1& 0.252+0.532=0.784\\\hline
    1&0& 0.288+0.128=0.416 \\\hline
    1&1& 0.504+0.304=0.808 \\\hline
    \end{array}



将 :math:`m_3(x_1,x_2)` 代入 :eq:`eq_7_3` 可得：

.. math::
    :label: eq_7_4

    p(x_1,\bar{x}_6) =p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)


同样的我们定义 :math:`m_2(x_1)=\sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)` ，计算可得：


.. math::
    m_4(x_2) m_3(x_1,x_2) =
    \begin{array}{|c|c|}
    \hline
    x_2&message\\
    \hline
    0&1\\
    \hline
    1&1\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}


.. math::

    p(x_2|x_1) [m_4(x_2) m_3(x_1,x_2)] =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&probability\\
    \hline
    0&0&0.4\\
    0&1&0.6\\
    \hline
    1&0&0.5\\
    1&1&0.5\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.368\\\hline
    0&1& 0.784\\\hline
    1&0& 0.416 \\\hline
    1&1& 0.808 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&x_2&message\\
    \hline
    0&0& 0.4\times 0.368=0.1472 \\\hline
    0&1& 0.6\times 0.784=0.4704\\\hline
    1&0& 0.5\times 0.416=0.208 \\\hline
    1&1& 0.5\times 0.808=0.404 \\\hline
    \end{array}


.. math::

    m_2(x_1)=\sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2) =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.1472+0.4704=0.6176\\\hline
    1& 0.208 + 0.404=0.612 \\\hline
    \end{array}


代入到 :eq:`eq_7_4` 可得：

.. math::
    :label: eq_7_5

    p(x_1,\bar{x}_6) =p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)
    = p(x_1)m_2(x_1)

    =
    \begin{array}{|c|c|}
    \hline
    x_1&probability\\
    \hline
    0&0.3\\
    \hline
    1&0.7\\
    \hline
    \end{array}
    \times
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.6176\\\hline
    1& 0.612 \\\hline
    \end{array}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.3\times0.6176 =0.18528  \\\hline
    1& 0.7\times0.612 =0.4284  \\\hline
    \end{array}



我们重新整理一下整个计算过程：

.. math::


    p(x_1,\bar{x}_6) &= \sum_{x_2,x_3,x_4,x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= \sum_{x_2} \sum_{x_3} \sum_{x_4} \sum_{x_5} p(x_1)p(x_2|x_1)p(x_3|x_1)p(x_4|x_2)p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) \sum_{x_5} p(x_5|x_3)p(\bar{x}_6|x_2,x_5)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) \sum_{x_4} p(x_4|x_2) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3) \sum_{x_4} p(x_4|x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)  m_4(x_2)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) \sum_{x_3} p(x_3|x_1) m_5(x_2,x_3)

    &= p(x_1) \sum_{x_2} p(x_2|x_1)  m_4(x_2) m_3(x_1,x_2)

    &=  p(x_1)m_2(x_1)



在这基础上，我们通过消除变量 :math:`\mathrm{x}_6` 计算出概率 :math:`p(\bar{x}_6)` ：

.. math::
    :label: eq_7_6

    p(\bar{x}_6=1) = \sum_{x_1}p(x_1)m_2(x_1) = 0.18528+0.4284=0.61368


然后根据贝叶斯定理，我们可以计算出条件概率 :math:`p(x_1|\bar{x}_6=1)` ：

.. math::

    p(x_1|\bar{x}_6=1) = \frac{p(x_1)m_2(x_1)}{\sum_{x_1}p(x_1)m_2(x_1)}
    =\frac{p(x_1)m_2(x_1)}{p(\bar{x}_6=1)}
    =
    \begin{array}{|c|c|c|}
    \hline
    x_1&message\\
    \hline
    0& 0.18528/0.61368 \approx 0.3 \\\hline
    1& 0.4284/0.61368  \approx 0.7 \\\hline
    \end{array}


我们发现 :eq:`eq_7_5` 中的边缘概率 :math:`p(x_1,\bar{x}_6)` 相当于是未归一化的条件概率 :math:`p(x_1|\bar{x}_6=1)` ，
通过 :eq:`eq_7_6` 的归一化常量 :math:`p(\bar{x}_6=1)` 可以计算出条件概率 :math:`p(x_1|\bar{x}_6)` 。

到此我们通过一个具体的例子演示了消元法的计算过程，消元法是概率图模型进行推断的直接算法，是一种精确的推断算法。
但是其有个明显的缺点就是，每一次概率查询(条件概率、边缘概率推断)都需要执行一次上述过程，比如，如果我们想要查询条件概率
:math:`p(x_1|\bar{x}_6=0),p(x_1|\bar{x}_4),\ldots` 等等，都需要分别执行一次上述过程，计算复杂度非常高。
然而，我们可以发现在执行不同的概率查询（对端）时，有些计算是重复的，
下一章节我们会讨论如何重复利用中间结果减少计算量的模型推断算法-和积算法(sum-product algorithm)。


.. _ch_condition_margin:

条件概率和边缘概率
===========================

为了能更直观的解释消元法，这里我们通过一些定义把条件变量取定值的操作也转化成求和操作，
通过这样的转化可以令条件变量和边际化消除变量具有相同的操作，更容易理解和操作。



我们令 :math:`\mathrm{x}_i` 表示证据变量，其观测值是 :math:`\bar{x}_i` 。
我们定义一个 *证据势函数(evidence potential)* ， :math:`\delta(x_i,\bar{x}_i)` ，
当 :math:`x_i=\bar{x}_i` 成立时这个函数值为1，否则为0。
通过这个函数我们可以把推断过程中对证据变量 :math:`\mathrm{x}_i` 的限制约束 (取值为 :math:`\bar{x}_i` ) 操作转化成求和操作。
函数 :math:`g(x_i)` 表示变量 :math:`\mathrm{x}_i` 的一个函数，比如在上面的例子中
:math:`g(x_6)=p(x_6|x_2,x_5)` ，通过下面的转换可以把 :math:`g(\bar{x}_6)=p(\bar{x}_6|x_2,x_5)` 转换成等价的求和操作。



.. math::

    g(\bar{x}_i)=\sum_{x_i} g(x_i) \delta(x_i,\bar{x}_i)
    = \sum_{x_i} p(x_6|x_2,x_5) \delta(x_i,\bar{x}_i)

这样在执行消元推断时，对于条件(证据)变量的限制约束操作转化为一个求和操作，二者的值是等价，
这样在上面的例子中就可以额外定义出 :math:`m_6(x_2,x_5)` 。

.. math::

    m_6(x_2,x_5) = \sum_{x_6} p(x_6|x_2,x_5) \delta(x_6,\bar{x}_6) = p(\bar{x}_6|x_2,x_5)




更一般的，我们可以扩展到多个条件变量的情形，我们用E表示条件变量集合，对于特定的条件(观测、证据)值
:math:`\bar{x}_E` ，我们想要计算 :math:`p(x_F|\bar{x}_E)` 。这时我们定义一个 *整体证据势函数(total evidence potential)* ：

.. math::

    \delta(x_E,\bar{x}_E) \triangleq \prod_{i\in E} \delta(x_i,\bar{x}_i)

只有当 :math:`x_E=\bar{x}_E` 成立时，这个函数为1，否则为0。通过这个势函数，我们可以把条件概率 :math:`p(x_F|\bar{x}_E)`
的分子分母都表示成求和的形式，分母其实就是分子的归一化，是在分子的基础上进行求和。

.. math::

    p(x_F,\bar{x}_E) = \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)

    p(\bar{x}_E) = \sum_{x_F} \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)

条件概率 :math:`p(x_F|\bar{x}_E)` 的计算公式为：

.. math::

    p(x_F|\bar{x}_E) &= \frac{p(x_F,\bar{x}_E)}{p(\bar{x}_E)}

    &= \frac{\sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)}{\sum_{x_F} \sum_{x_E} p(x_F,x_E)\delta(x_E,\bar{x}_E)}


条件概率 :math:`p(x_F|\bar{x}_E)` 的分母 :math:`p(\bar{x}_E)` 是其分子 :math:`p(x_F,\bar{x}_E)` 的累加求和，
也就是说其实只要计算出分子部分，分母就自然得到了，从某种角度上讲只要计算出 :math:`p(x_F,\bar{x}_E)` 就相当于计算出
条件概率 :math:`p(x_F|\bar{x}_E)` ，那么我们可以把 :math:`p(x_F,\bar{x}_E)` 看成是条件概率令一种表示。
然而 :math:`p(x_F,\bar{x}_E)` 本身又是在边缘概率 :math:`p(x_F,x_E)` 的基础上加了一个证据势函数
:math:`p(x_F,\bar{x}_E)=p(x_F,x_E)\delta(x_E,\bar{x}_E)` 。我们可以用不加修改的消元法计算
:math:`p(x_F,x_E)` 和 :math:`p(x_F,\bar{x}_E)` ，本质上就是模型的条件概率和边缘概率的推断问题可以看成是等价的。
两者都是进行边际化消除，计算逻辑是一样的，不同的是 :math:`p(x_F,\bar{x}_E)` 多了一个证据势函数 :math:`\delta(x_E,\bar{x}_E)` 。

**通过引入证据势函数，我们把条件变量的值限定操作转换成了求和消除操作，条件变量可以和其他被边缘化消除的变量在操作上同等看待，**
**这样一来在图模型上进行条件概率查询也可以看做是进行边缘概率查询，因为两者在计算上是等价的。**
**也就是说我们可以把** :math:`p(x_F|\bar{x}_E)` 和 :math:`p(x_F,x_E)` **都当成是在求"边缘概率"，**
**在图模型的推断算法的讨论中，我们将不再区分两者，都会按照边缘概率查询来讨论。**
这样的方式同样适用于无向图，对于证据变量集合E，为其中每个结点的局部势函数 :math:`\psi_i(x_i)` 乘上 :math:`\delta(x_i,\bar{x}_i)` 。

.. math::
    \psi_i^E (x_i) \triangleq \psi_i(x_i)\delta(x_i,\bar{x}_i) ,i \in E




一个包含证据(观测值)值的有向图的条件概率，可以用的联合（边缘）概率的形式表示，其中E表示证据变量集合：

.. math::

    p^E(x) \triangleq p(x) \delta (x_E,\bar{x}_E)


对于无向图可以有同样的定义：

.. math::

    p^E(x) \triangleq \frac{1}{Z} \prod_{C \in \mathcal{C}} \psi^E_{\mathrm{x}_C} (x_C)


有向图的消元法
===========================

在上一节的例子中我们已经展示了有向图的消元算法的过程，这里我们在重新整理一下有向图中的消元算法。

这个算法执行过程中的每一步都是在一个因子函数乘积上执行一个求和消元的过程，
这些因子函数可以是局部条件概率 :math:`p(x_i|x_{\pi_i})` 、证据势函数 :math:`\delta(x_i,\bar{x}_i)`
、中间信息因子 :math:`m_i(x_{S_i})` 。所有的这些函数都是定义在局部结点子集上的，这里统一用 "势函数(potential function)" 表示。
所以消元算法其实是一个在势函数的乘积上面通过求和消除变量的过程。

我们整理一下有向图中消元算法的伪代码过程：


    消元整体过程( :math:`\mathcal{G},E,F` )

        过程1： 初始化图和查询变量( :math:`\mathcal{G},F` )

        过程2：引入证据( :math:`E`  )

        过程3：更新( :math:`\mathcal{G}` )

        过程4：归一化查询变量(F)

    过程1. 初始化图和查询变量( :math:`\mathcal{G},F` )

        选择一个消元的顺序 :math:`I` ，F 变量排在最后。

        **foreach** :math:`\mathrm{x}_i` **in** :math:`\mathcal{V}` :

            把 :math:`p(x_i|x_{\pi_i})` 放到激活列表中  // *生成联合概率因子分解的过程*

        **end for**

    过程2. 引入证据( :math:`E`  )

        **foreach** i **in** E:
            把 :math:`\delta(x_i,\bar{x}_i)` 加入到激活列表中

        **end for**

    过程3：更新( :math:`\mathcal{G}` )

        **foreach** i **in** :math:`I` :

            从激活列表中找到所有包含 :math:`x_i` 的势函数从激活列表中去掉这下势函数

            令 :math:`\phi_i(x_{T_i})` 表示这些势函数的乘积

            令 :math:`m_i(x_{S_i})=\sum_{x_i} \phi_i(x_{T_i})`

            将 :math:`m_i(x_{S_i})` 加入到激活列表中

        **end for**

    过程4：归一化查询变量(F)

        :math:`p(x_F|\bar{x}_E) \leftarrow \frac{\phi_F(x_F)}{\sum_{x_F} \phi_F(x_F)}`



注意，在上述伪代码流程中我们定义了符号 :math:`T_i=\{i\}\cup S_i` ，表示的是求和子项 :math:`\sum_{x_i}`
中包含的全部结点子集。当消元过程执行到只剩下查询变量 :math:`\mathrm{x}_F` 时算法结束，这时我们就得到了未归一化的
"条件概率" :math:`p(x_F,\bar{x}_E)` ，通过在其上面对 :math:`x_F` 求和可以得到归一化因子 :math:`p(\bar{x}_E)` 。
让我们回到上面的例子中，按照伪代码的过程阐述一遍。

**过程1，初始化图和查询变量。**
首先我们确定证据结点 :math:`\mathrm{x}_6` ，查询结点是 :math:`\mathrm{x}_1` 。
选定消元顺序 :math:`I=(6,5,4,3,2,1)` ，其中查询结点排在最后面。
然后把所有局部条件概率放到激活列表中
:math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),p(x_6|x_2,x_5)\}` 。

**过程2. 引入证据。**
把证据势函数 :math:`\delta (x_6,\bar{x}_6)`
追加到激活列表中

.. math::

    \{ p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),p(x_6|x_2,x_5),\delta(x_6,\bar{x}_6) \}


**过程3：更新。**
    首先消除结点 :math:`\mathrm{x}_6` ，激活列表中包含变量 :math:`\mathrm{x}_6` 的"势函数(potential function)"有
    :math:`p(x_6|x_2,x_5)` 和 :math:`\delta(x_6,\bar{x}_6)` ，所以我们有
    :math:`\phi_6(x_2,x_5,x_6)=p(x_6|x_2,x_5)\delta(x_6,\bar{x}_6)` ，对 :math:`x_6` 进行求和得到
    :math:`m_6(x_2,x_5)=p(\bar{x}_6|x_2,x_5)` 。把这个新的势函数加入到激活列表中，并且从激活列表中移除
    :math:`p(x_6|x_2,x_5)` 和 :math:`\delta(x_6,\bar{x}_6)` 。至此，我们就完成了证据的引入，把
    :math:`p(x_6|x_2,x_5)` 限定为 :math:`\bar{x}_6` 。此时激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),p(x_5|x_3),m_6(x_2,x_5)\}`

    现在开始消除变量 :math:`\mathrm{x}_5` ，激活列表中包含变量 :math:`\mathrm{x}_5` 的势函数有
    :math:`p(x_5|x_3)` 和 :math:`m_6(x_2,x_5)` ，移除它们，然后定义
    :math:`\phi_5(x_2,x_3,x_5)=p(x_5|x_3)m_6(x_2,x_5)` ，对 :math:`\mathrm{x}_5` 进行求和得到
    :math:`m_5(x_2,x_3)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),p(x_4|x_2),m_5(x_2,x_3)\}` 。

    现在开始消除变量 :math:`\mathrm{x}_4` ，激活列表中包含变量 :math:`\mathrm{x}_4` 的势函数有
    :math:`p(x_4|x_2)` ，移除它，然后定义
    :math:`\phi_4(x_2,x_4)=p(x_4|x_2)` ，对 :math:`\mathrm{x}_4` 进行求和得到
    :math:`m_4(x_2)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),p(x_3|x_1),m_4(x_2),m_5(x_2,x_3)\}` 。


    现在开始消除变量 :math:`\mathrm{x}_3` ，激活列表中包含变量 :math:`\mathrm{x}_3` 的势函数有
    :math:`p(x_3|x_1)` 和 :math:`m_5(x_2,x_3)` ，移除它们，然后定义
    :math:`\phi_3(x_1,x_2,x_3)=p(x_3|x_1)m_5(x_2,x_3)` ，对 :math:`\mathrm{x}_3` 进行求和得到
    :math:`m_3(x_1,x_2)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)\}` 。


    现在开始消除变量 :math:`\mathrm{x}_2` ，激活列表中包含变量 :math:`\mathrm{x}_2` 的势函数有
    :math:`p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)` ，移除它们，然后定义
    :math:`\phi_2(x_1,x_2)=p(x_2|x_1),m_4(x_2),m_3(x_1,x_2)` ，对 :math:`\mathrm{x}_2` 进行求和得到
    :math:`m_2(x_1)` ，加入到激活列表中。此时，激活列表为
    :math:`\{p(x_1),m_2(x_1)\}` 。

**过程4：归一化查询变量。**
    现在我们得到了 :math:`\phi_1(x_1)=p(x_1)m_2(x_1)` ，这其实就是"未归一化的条件概率" :math:`p(x_1,\bar{x}_6)` ，
    在其基础上消除 :math:`x_1` 得到 :math:`m_1=\sum_{x_1} \phi_1(x_1)` 就是归一化因子 :math:`p(\bar{x}_6)` 。




无向图的消元法
===========================

有向图的消元算法同样也适用于无向图，并不需要过多改变。唯一的变化就是激活列表中有向图的局部条件概率变成无向图的势函数
:math:`\{\psi_{\mathrm{x}_C}(x_C)\}`。
让我们考虑一个无向图的示例 ，这个无向图是上节的有向图转化而来。




.. _fg_7_a3:
.. figure:: pictures/7_a3.jpg
    :scale: 40 %
    :align: center

    无向图示例，深色阴影结点 :math:`\mathrm{x}_6` 是条件变量；浅色阴影结点，
    :math:`\{\mathrm{x}_2,\mathrm{x}_3,\mathrm{x}_4,\mathrm{x}_5\}` ，是需要边际化消除的变量集合；
    :math:`\mathrm{x}_1` 是查询结点。

如 :numref:`fg_7_a3` ，我们继续以查询条件概率 :math:`p(x_1|\bar{x}_6)` 为例，我们用定义在团上的势函数表示这个无向图的联合概率分布，
图上的团有 :math:`\{\mathrm{x_1},\mathrm{x_2}\},\{\mathrm{x_1},\mathrm{x_3}\},\{\mathrm{x_2},\mathrm{x_4}\},\{\mathrm{x_3},\mathrm{x_5}\},\{\mathrm{x_2},\mathrm{x_5},\mathrm{x_6}\}`
，则这个无向图的联合概率分布为：

.. math::

    p_{\mathbf{x}}(\mathbf{x}) =\frac{1}{Z} \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4) \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6)


类似于有向图的推断过程，首先我们计算未归一化的条件概率 :math:`p(x_1,\bar{x}_6)` 。

.. math::

    p(x_1,\bar{x}_6) &= \frac{1}{Z} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6) \delta(x_6,\bar{x}_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) \sum_{x_6} \varphi_{256}(x_2,x_5,x_6) \delta(x_6,\bar{x}_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) m_6(x_2,x_5)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) \sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) m_3(x_1,x_2)

     &= \frac{1}{Z} m_2(x_1)


对 :math:`x_1` 进行求和边际化可得到归一化因子：

.. math::

    p(\bar{x}_6) = \frac{1}{Z} \sum_{x_1} m2(x_1)

然后我们就能得到想要查询的条件概率 :math:`p(x_1|\bar{x}_6)` ：

.. math::

    p(x_1|\bar{x}_6) = \frac{m_2(x_1)}{\sum_{x_1} m_2(x_1)}


**我们发现无向图的归一化系数Z无需计算出来，在进行条件概率查询时可以消除掉。**
然而当我们需要计算边缘概率 :math:`p(x_i)` 时，这个系数Z就无法被消除了，需要被准确的计算出来。
但也不是很困难，**系数Z其实就是对联合概率分布中全部变量进行求和消除的结果** ，
我们在求边缘概率 :math:`p(x_i)` 时，已经在执行变量消除了，
消除了除变量 :math:`x_i` 以外的所有结点得到 :math:`m_i(x_i)` ，
这时有 :math:`Z=\sum_{x_i} m_i(x_i)` 。
具体举例说明下，还是这个无向图，假设想要查询边缘概率 :math:`p(x_1)` ，不是条件概率，没有证据变量，也就不需要引入 :math:`\delta(\cdot)`

.. math::

    p(x_1) &= \frac{1}{Z} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) \sum_{x_6} \varphi_{256}(x_2,x_5,x_6)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4) \sum_{x_5} \varphi_{35}(x_3,x_5) m_6(x_2,x_5)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2)\sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)
    \sum_{x_4} \varphi_{24}(x_2,x_4)

    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) \sum_{x_3} \varphi_{13}(x_1,x_3) m_5(x_2,x_3)


    &= \frac{1}{Z} \sum_{x_2} \varphi_{12}(x_1,x_2) m_4(x_2) m_3(x_1,x_2)

    &= \frac{1}{Z} m_2(x_1)

这时有：

.. math::
    Z= \sum_{x_1} \sum_{x_2}\sum_{x_3}\sum_{x_4}\sum_{x_5}\sum_{x_6}
    \varphi_{12}(x_1,x_2) \varphi_{13}(x_1,x_3) \varphi_{24}(x_2,x_4)
    \varphi_{35}(x_3,x_5) \varphi_{256}(x_2,x_5,x_6) =\sum_{x_1} m_2(x_1)





无向图的消元算法和有向图本质上是一样的，**消元法的本质就是找到一个合适顺序进行边际化消除变量。**



图消除
########################################################

.. todo::
    待补充



总结
########################################################


- 概率图推断通常关注两个问题: a.边缘概率查询；b.条件概率查询，有时也称为后验概率。
- 消元法的本质就是找到一个合适的顺序进行变量的边际化(离散变量求和，连续变量积分)。
- 消元法的相比原始方法提高了计算效率。


