信息论基础
###################################################
信息论是一门用数理统计方法来研究信息的度量、传递和变换规律的科学。
它主要是研究通讯和控制系统中普遍存在着信息传递的共同规律以及研究最佳解决信息的获限、度量、变换、储存和传递等问题的基础理论。
这似乎与概率论和机器学习的关注点相去甚远，但实际上两者之间有着密切的联系。
因为简单地表示数据，需要将短码字分配给高概率的位串，并将长码字保留给低概率的位串。
这与自然语言中的情况类似，常见的单词(如“a”、“the”、“and”)通常比罕见的单词短得多。
这种情况下需要一个模型来预测哪些数据是可能的，哪些是不可能的，这也是机器学习的一个中心问题。

信息熵
===================================================
符合分布 :math:`p` 的随机变量 :math:`X` 的熵，表示为 :math:`H(X)`，是度量随机变量不确定性的指标。
对于有 :math:`K` 个状态的离散随机变量，可以定义为：

.. math::

    H(X) = - \sum_{k=1}^{K}p(X = k)\log_{2}p(X=k)

通常使用以2为底的对数。对于离散随机变量符合均匀分布的时候信息熵最大。
因此，对于有 :math:`K` 个状态的离散随机变量，如果 :math:`p(x=k) = 1 / K`，此时的信息熵最大。
相反，最小的信息熵是随机变量的分布没有不确定性，此时的信息熵为0。

对于符合伯努利分布的随机变量 :math:`X \in \{0, 1\}`，也可以表示为 :math:`p(X=1) = \theta` 和 :math:`p(X=0) = 1 - \theta`。
因此，信息熵可以表示为：

.. math::

    H(X) &= -[p(X = 1)\log_{2}p(X=1) + p(X = 0)\log_{2}p(X=0)]

    &= -[\theta\log_{2}\theta + (1-\theta)\log_{2}(1-\theta)]


当 :math:`\theta=0.5` 时，也就是随机变量符合均匀分布，此时有最大的信息熵为1。


数据编码
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

数据编码在某些情况下需要对数据进行压缩，使用最有效的编码表示原始的数据。

压缩就是找出文件内容的概率分布，将那些出现概率高的部分代替成更短的形式。所以，内容越是重复的文件，就可以压缩地越小。

相应地，如果内容毫无重复，就很难压缩。极端情况就是，遇到那些均匀分布的随机字符串，往往连一个字符都压缩不了。

压缩就是一个消除冗余的过程，相当于用一种更精简的形式，表达相同的内容。而信息熵可以看作是数据压缩的一个临界值。

一般情况下，假设文件中的字符符合均匀分布，而一个字符在文件中出现的概率为 :math:`p`，那么在这个位置上最多可能出现 :math:`1/p` 种情况，
也就是需要 :math:`log_{2}\frac{1}{p}` 个比特表示文件中的字符。

假设一个文件由 :math:`n` 个部分组成，每个部分在文件中出现的概率为 :math:`p_{i}`，则压缩文件所需的比特数至少为：

.. math::
    log_{2}(\frac{1}{p_{1}}) + log_{2}(\frac{1}{p_{2}}) + ... + log_{2}(\frac{1}{p_{n}}) = \sum log_{2}(\frac{1}{p_{i}})

则平均每个部分所占用的比特数为，即：

.. math::
    p_{1} * log_{2}(\frac{1}{p_{1}}) + p_{2} * log_{2}(\frac{1}{p_{2}}) + ... + p_{n} * log_{2}(\frac{1}{p_{n}}) = \sum p_{i} * log_{2}(\frac{1}{p_{i}}) = E(log_{2}(\frac{1}{p}))

霍夫曼编码就是利用了这种大概率事件分配短码的思想，而且可以证明这种编码方式是最优的。
霍夫曼编码使用变长编码表对源符号（如文件中的一个字母）进行编码，其中变长编码表是通过一种评估来源符号出现机率的方法得到的，
出现机率高的字母使用较短的编码，反之出现机率低的则使用较长的编码，这便使编码之后的字符串的平均长度、期望值降低，从而达到无损压缩数据的目的。

KL散度
===================================================
两个概率分布 :math:`p` 和 :math:`q` 的差异度量指标可以使用KL散度，或者称之为相对熵，定义为：

.. math::

   KL(p||q) &= \sum_{k}p_{k}\log \frac{p_{k}}{q_{k}}

            &= \sum_{k}p_{k}\log p_{k} - \sum_{k}p_{k}\log q_{k}

            &= -H(p) + H(p, q)

KL散度不是距离，因为它不是对称的，KL散度的对称版本是JS散度，定义为：

.. math::

   JS(p_1, P_2) = 0.5KL(p_1 || q) + 0.5KL(p_2 || q), \ q = 0.5p_1 + 0.5p_2

:math:`H(p, q) = \sum_{k}p_{k}\log q_{k}` 称之为交叉熵。
交叉熵是指使用分布 :math:`q` 编码来自分布 :math:`p` 的数据所需的平均比特数。

KL散度是指使用分布 :math:`q` 编码来自分布 :math:`p` 的数据所需的平均额外比特数。
自然地，:math:`KL(p||q) \geq 0`，并且只有当 :math:`p=q` 的时候KL散度等于0。

互信息
===================================================

考虑两个随机变量 :math:`X` 和 :math:`Y`，假设想知道一个变量对另一个变量的了解程度，可以计算相关系数，但这只对实值随机变量有定义，而且，这是一个非常有限的相关性度量。
一个更一般的方法是确定联合概率分布 :math:`p(X, Y)` 和分布 :math:`p(X)p(Y)` 的相似程度，这被称之为互信息，定义为：

.. math::

  I(X; Y) = KL(p(X, Y) || p(X)p(Y)) = \sum_{x}\sum_{y}p(x, y)\log \frac{p(x, y)}{p(x)p(y)}

这里的 :math:`I(X, Y) \geq 0`，并且在 :math:`p(X, Y) = p(X)p(Y)` 时取等号，意味着随机变量相互独立的时候互信息为0。

为了深入理解互信息的含义，可以用联合熵和条件熵来重新表达互信息：

.. math::

  I(X; Y) = H(X) - H(X| Y) = H(Y) - H(Y|X)

:math:`H(Y|X)` 是条件熵，定义为 :math:`H(Y|X)=\sum_{x}p(x)H(Y|X =x)`，因此，可以将 :math:`X` 和 :math:`Y` 之间的互信息解释为观察 :math:`Y` 后 :math:`X` 的不确定性的减少，
或者说是观察 :math:`X` 后 :math:`Y` 的不确定性的减少。

连续型随机变量的互信息
>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

对于连续型随机变量，首先需要离散化或者量化，具体是通过将每个变量的范围划分到不同的箱中，然后计算多少值落入每个箱中，最后再用离散随机变量的方式计算互信息。
不幸的是，使用箱的数目，在每个箱边界的位置，可能对最后的结果产生重大影响。

解决此问题的一种方法是尝试直接估计MI，而无需先执行密度估计。另一种方式是尝试多种不同大小和位置的箱，得到其中最大的互信息。