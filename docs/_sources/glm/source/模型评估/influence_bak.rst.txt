#########################################
模型检验
#########################################





我们基于样本训练模型，基于样本计算模型拟合优度指标，并给出模型好坏的结论。
然而，这一切都是建立随机样本的基础上，模型拟合优度指标也是一个随机量，
我们的结论是根据样本推断(influence)得出的，推断得出结论不是百分百准确的，
这就需要同时给出这个结论的可靠程度，而这就是统计推断(statistical inference)所做的事情。



上一章我们介绍了 ``GLM`` 中评价模型拟合好坏程度的常见指标，以及这些指标的定义和计算方法，
但是没有说明如何根据指标值得出结论，本章我们探讨如何根据拟合优度指标的值得出模型优劣的结论，
以及结论的可靠程度。再讨论 ``GLM`` 推断方法前，先简单讲解一下统计学中的推断和检验的理论，
``GLM`` 的推断过程就是统计学推断理论的一个应用。








``GLM`` 中的抽样分布
#########################################

无论是置信区间还是假设检验都需要知道统计量的抽样分布，
因此要对 ``GLM`` 拟合优度进度推断和检验，就要知道各个拟合优度指标的抽样分布。
本节我们推导一下 ``GLM`` 中一些关键统计量的抽样分布。

如果响应变量是正态分布，则通常可以准确确定一些统计量的抽样分布。
反之，如果响应变量不是正态分布，就需要依赖中心极限定理，找到其大样本下的近似分布。
注意，这些结论的成立都是有一些前提条件的，
对于来自属于指数族分布的观测数据，特别是对于广义线性模型，确实满足了必要条件。
在本节我们只给出统计量抽样分布的一些关键步骤，
Fahrmeir和Kaufmann（1985）给出了广义线性模型抽样分布理论的详细信息。


如果一个统计量 :math:`S`，其渐近服从正态分布 :math:`S \sim \mathcal{N}(\mathbb{E}[S],V(S))`
，其中 :math:`\mathbb{E}[S]` 和 :math:`V(S)` 分别是 :math:`S` 的期望和方差
，则近似的有：


.. math::

    \frac{S-\mathbb{E}[S]}{\sqrt{V(S)}} \sim \mathcal{N}(0,1)


根据卡方分布的定义，等价的有

.. math::
    \frac{(S-\mathbb{E}[S])^2}{V(S)} \sim \chi^2 (1)


如果 :math:`S` 是一个向量 :math:`\pmb{S}^{T}=[S_1,\dots,S_k]` ，上述结论可以写成向量的模式。

.. math::
    :label: eq_influence_110

    (\pmb{S}-\mathbb{E}[\pmb{S}])^T \pmb{V}^{-1}(\pmb{S}-\mathbb{E}[\pmb{S}])
    \sim \chi^2 (k)

其中 :math:`\pmb{V}` 是协方差矩阵，并且必须是非奇异矩阵。


得分统计量
=================================

我们已经知道似然函数及其一阶导数都是一个关于样本的函数，
所以似然函数及其一阶导数都是统计量(statistic)。
似然函数的一阶导数又叫做得分函数(score function)，也称为得分统计量(score statics)。
假设 :math:`Y_1,\dots,Y_N` 是相互独立的 ``GLM`` 样本变量，
这里我们强调 :math:`Y_i` 是一个随机变量，所以用大写符号表示。
其中有 :math:`\mathbb{E}[Y_i]=\mu_i` , :math:`g(\mu_i)=\beta^T x_i=\eta_i`
，自然参数 :math:`\theta_i` 是一个关于 :math:`\mu_i` 函数。
``GLM`` 模型的对数似然函数为

.. math::

    \ell= \sum_{i=1}^N \left \{   \frac{Y_i \theta_i - b(\theta_i)}{a(\phi)}   + c(y_i,\phi)   \right \}





根据 :numref:`章节%s <ch_glm_estimate>` 的内容， ``GLM`` 对数似然函数的一阶导数，得分统计量为：

.. math::
    U_j = \frac{ \partial \ell}{\beta_j}
    &= \sum_{i=1}^N \left ( \frac{\partial \ell_i}{\partial \theta_i} \right )
    \left ( \frac{\partial \theta_i}{\partial \mu_i} \right )
    \left ( \frac{\partial \mu_i}{\partial \eta_i} \right )
    \left ( \frac{\partial \eta_i}{\partial \beta_j} \right )

    &= \sum_{i=1}^N \left \{ \frac{Y_i-b'(\theta_i)}{a(\phi)}   \right \}
    \left \{ \frac{1}{\nu(\mu_i)} \right \} \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}

    &= \sum_{i=1}^N \frac{Y_i-\mu_i}{a(\phi) \nu(\mu_i) } \left ( \frac{\partial \mu}{\partial \eta} \right )_i x_{ij}

    &= \sum_{i=1}^N \frac{Y_i-\mu_i}{V(Y_i) } \left ( \frac{\partial \mu_i}{\partial \eta_i} \right ) x_{ij}

注意下标 :math:`j` 表示的参数向量的下标，:math:`U_j` 是 :math:`\beta_j` 的一阶偏导数。
对于任意的样本 :math:`Y_i` 都有 :math:`\mathbb{E}[Y_i]=\mu_i`
，因此有：

.. math::
    \mathbb{E}_{Y_i}[U_j] = 0

:math:`U` 的协方差矩阵就是信息矩阵 :math:`\mathcal{J}`。

.. math::
    \mathcal{J}_{jk} = \mathbb{E}[U_jU_k]




在  :numref:`章节%s参数估计 <ch_estimate>` 我们讲过，
信息矩阵 :math:`\mathcal{J}` 又等于对数似然函数二阶偏导数的期望的负数，

.. math::

    \mathcal{J} = - \mathbb{E}[\ell'']
    = - \mathbb{E}[U']


如果模型的参数向量 :math:`\beta` 只有一个截距参数， :math:`\beta=[\beta_0]` ，
此时模型只有一个参数，得分统计量 :math:`U` 是一个标量，其渐近服从正态分布。

.. math::
    :label: eq_glm_influence_015

    U  \sim \mathcal{N}(0,\mathcal{J})
    \ \text{或者} \
    \frac{U}{\sqrt{\mathcal{J}}} \sim \mathcal{N}(0,1)

根据卡方分布的定义，也可以写成

.. math::

     \frac{U^2}{\mathcal{J}}  \sim \chi^2 (1)


如果 :math:`\beta` 是一个参数向量，:math:`\beta^T=[\beta_0,\beta_1,\dots,\beta_p]`，
模型一共有 :math:`p+1` 个参数，
则 :math:`\textbf{U}` 表示一个向量 :math:`\textbf{U}^T=[U_0,U_1,\dots,U_p]`
，此时 :math:`\textbf{U}` 渐近服从多维正态分布(multivariate Normal distribution,MVN)。

.. math::

    \textbf{U} \sim MVN(\textbf{0},\mathbf{\mathcal{J}})

在大样本下有

.. math::

    \textbf{U}^T \mathbf{\mathcal{J}}^{-1} \textbf{U} \sim  \chi^2 (p+1)

高斯分布的得分统计量
-----------------------------------------------

令 :math:`Y_1,\dots,Y_N` 是独立同分布的高斯随机变量，:math:`Y_i \sim \mathcal{N}(\mu,\sigma^2)`
，其中 :math:`\mu` 是未知的， :math:`\sigma^2` 是已知的常量，
并且所有变量 :math:`Y_i` 都是拥有同样的均值参数 :math:`\mu` 和常量方差 :math:`\sigma^2` 。
其对数似然函数为：


.. math::

    \ell(\mu;Y,\sigma) = -\frac{1}{2\sigma^2} \sum_{i=1}^N (Y_i -\mu)^2 - N \ln (\sigma \sqrt{2\pi})


其得分统计量为


.. math::
    :label: eq_glm_influence_020

    U = \frac{d \ell}{d \mu} = \frac{1}{\sigma^2} \sum_{i=1}^N (Y_i -\mu)

可以把样本均值统计量 :math:`\sum_{i=1}^N Y_i = N \bar{Y}` 代入到 :math:`U` ，


.. math::

    U = \frac{1}{\sigma^2} ( N \bar{Y} - N \mu)
    =\frac{N}{\sigma^2} (\bar{Y} - \mu)


通过令 :math:`U=0` 可以得到参数 :math:`\mu` 的最大似然估计量 :math:`\hat{\mu}=\bar{Y}`
。


现在看下统计量 :math:`U` 的期望和方差，其期望是

.. math::

    \mathbb{E}[U] = \frac{N}{\sigma^2} ( \mathbb{E}[\bar{Y}] - \mu)
    = \frac{N}{\sigma^2} ( \mu - \mu) =0


统计量 :math:`U` 的方差是

.. math::
    :label: eq_glm_influence_021

    V(U)
    &= V \left[   \frac{1}{\sigma^2} \sum_{i=1}^N (Y_i -\mu)  \right]

    &=\frac{1}{\sigma^4}  V \left[  \sum_{i=1}^N (Y_i -\mu)  \right]

    &= \frac{1}{\sigma^4}  \sum_{i=1}^N V(Y_i)

    &= \frac{N}{\sigma^2}

    &= \mathcal{J}

结合 :eq:`eq_glm_influence_020` 和 :eq:`eq_glm_influence_021` 有

.. math::

    \frac{U}{\sqrt{\mathcal{J}}} = \frac{\sqrt{N}(\bar{Y} - \mu)}{\sigma}





二项分布的得分统计量
-----------------------------------------------

现在假设 :math:`Y_i \sim Bin(n,\pi)`，
对数似然函数为

.. math::

    \ell(\pi;y) = \sum_{i=1}^N \left [ Y_i \ln \pi +(n-Y_i) \ln (1-\pi) + \ln \binom{n}{Y_i}
    \right ]

得分统计量是

.. math::
    U &=\frac{d \ell}{d \pi}

    &=\sum_{i=1}^N \left [ \frac{Y_i}{\pi} - \frac{n-Y_i}{1-\pi} \right ]

    &= \sum_{i=1}^N  \frac{Y_i-n\pi}{\pi(1-\pi)}

    &= \frac{1}{\pi(1-\pi)} \sum_{i=1}^N  (Y_i-n\pi)

然后代入样本均值统计量， :math:`\sum_{i=1}^N Y_i = N\bar{Y}` ，可以把 :math:`U` 改写成

.. math::

    U = \frac{ N(\bar{Y} - n\pi)}{\pi(1-\pi)}


因为 :math:`\mathbb{E}[Y_i]=n\pi` ，
所以 :math:`\mathbb{E}[U]=0`。
又因为 :math:`V(Y_i) = n\pi(1-\pi)`
，所以

.. math::

    V(U)
    &=V\left[     \sum_{i=1}^N  \frac{Y_i-n\pi}{\pi(1-\pi)}      \right ]

    &= \frac{1}{\pi^2(1-\pi)^2} V \left [ \sum_{i=1}^N (Y_i-n\pi)      \right]

    &= \frac{1}{\pi^2(1-\pi)^2} \sum_{i=1}^N V(Y_i-n\pi)

    &= \frac{1}{\pi^2(1-\pi)^2} \sum_{i=1}^N V(Y_i)

    &= \frac{Nn}{\pi(1-\pi)}

    &= \mathcal{J}


因此有

.. math::


    \frac{U}{\sqrt{J}} = \frac{\sqrt{N}(\bar{Y}-n\pi)}{\sqrt{n\pi(1-\pi)}}
    \sim \mathcal{N}(0,1)


.. _ch_influence_ml_statistic:

参数估计量
=================================


在讨论参数估计量的抽样分布前，先回顾一下泰勒级数近似(Taylor series approximation)，
后续统计量抽样分布的推导依赖泰勒级数。

.. topic:: 泰勒级数

    定义一个单变量的函数 :math:`f(x)`，
    对于函数上的某个点 :math:`x=t` 的附近有如下近似成立：

    .. math::

        f(x) = f(t) + (x-t)\left[ \frac{df}{dx} \right]_{x=t}
        + \frac{1}{2}(x-t)^2 \left[ \frac{d^2f}{d x^2}  \right ]_{x=t}
        + \dots

本节我们利用泰勒级数推导 ``GLM`` 中协变量参数 :math:`\beta`
的似然估计量 :math:`\hat{\beta}` 的抽样分布。
为了便于理解，我们先用空模型进行推导，然后再扩展到一般模型。

**空模型**

对于空模型，模型只有一个截距参数 :math:`\beta=[\beta_0]`，
假设截距参数 :math:`\beta_0` 的估计值是 :math:`\hat{\beta}`
，在估计值 :math:`\hat{\beta}` 的附近用泰勒级数展开为

.. math::

    \ell(\beta) = \ell(\hat{\beta}) + (\beta-\hat{\beta})U(\hat{\beta})
    + \frac{1}{2}(\beta-\hat{\beta})^2 U'(\hat{\beta})

其中 :math:`U(\hat{\beta})` 表示 :math:`\ell(\beta=\hat{\beta})` 的一阶导数，
用 :math:`U'(\hat{\beta})` 表示 :math:`\ell(\beta=\hat{\beta})` 的二阶导数。
我们知道 :math:`\mathbb{E}[U']=-\mathcal{J}`
，现在我们用 :math:`U'(\hat{\beta})` 的期望值代 :math:`-\mathcal{J}(\hat{\beta})` 替其自身，

.. math::
    \ell(\beta) = \ell(\hat{\beta}) + (\beta-\hat{\beta})U(\hat{\beta})
    - \frac{1}{2}(\beta-\hat{\beta})^2 \mathcal{J}(\hat{\beta})




现在我们把得分函数 :math:`U` 在 :math:`\beta=\hat{\beta}` 的附近展开，
但这里我们只要前两项，忽略二阶以及更高阶的项。

.. math::

    U(\beta) = U(\hat{\beta}) + (\beta-\hat{\beta}) U'(\hat{\beta})

同样，可以用 :math:`-\mathcal{J}` 代替 :math:`U'(\hat{\beta})`，

.. math::
    :label: eq_me_122

    U(\beta) = U(\hat{\beta}) - (\beta-\hat{\beta}) \mathcal{J}(\hat{\beta})




**一般模型**

如果是一般模型，模型的协变量参数 :math:`\beta` 是一个向量，
上述推导过程仍然适用，只需要把标量参数改成向量参数即可。

.. math::
    :label: eq_influence_250

    \ell(\pmb{\beta}) = \ell(\pmb{\hat{\beta}}) + (\pmb{\beta}-\pmb{\hat{\beta}})^T \mathbf{U}(\pmb{\hat{\beta}})
    - \frac{1}{2}(\pmb{\beta}-\pmb{\hat{\beta}})^T \pmb{\mathcal{J}}(\pmb{\hat{\beta}})(\pmb{\beta}-\pmb{\hat{\beta}})


:eq:`eq_me_122` 的向量版本为


.. math::
    :label: eq_influence_251

    \mathbf{U}(\pmb{\beta}) = \mathbf{U}(\pmb{\hat{\beta}}) -
    (\pmb{\beta}-\pmb{\hat{\beta}}) \pmb{\mathcal{J}}(\pmb{\hat{\beta}})


标量和向量在公式以及推导上没有本质区别，所以后续不再用粗体进行区分，默认都是向量。



公式中 :math:`U(\hat{\beta})` 是对数似然函数的在 :math:`\hat{\beta}` 处的一阶导数，
而 :math:`\hat{\beta}` 是通过令 :math:`U(\beta)=0` 得到似然估计值，
显然有 :math:`U(\hat{\beta})=0` 成立。
因此 :eq:`eq_me_122` 或者 :eq:`eq_influence_251` 可以简写成

.. math::
    :label: eq_glm_influence_085


        U(\beta) = - (\beta-\hat{\beta}) \mathcal{J}(\hat{\beta})
        = (\hat{\beta}-\beta) \mathcal{J}(\hat{\beta})

根据上一节的结论（:eq:`eq_glm_influence_015`），统计量 :math:`U/\sqrt{\mathcal{J}}`
的抽样分布是标准高斯分布。

.. math::
    :label: eq_glm_influence_086

    \frac{U}{\sqrt{\mathcal{J}}} \sim \mathcal{N}(0,1)


:eq:`eq_glm_influence_085` 代入 :eq:`eq_glm_influence_086` 可得

.. math::
    :label: eq_glm_influence_087

    \frac{(\hat{\beta}-\beta) \mathcal{J}(\hat{\beta}) }{ \sqrt{\mathcal{J}(\hat{\beta})} }
    = \frac{(\hat{\beta}-\beta)  }{ \sqrt{\mathcal{J}(\hat{\beta})^{-1} } }
    \sim \mathcal{N}(0,1)





我们知道 :math:`\mathbb{E}[U]=0` ，
如果把 :math:`\mathcal{J}` 看做一个常量，:math:`\beta` 是参数的真实值，
则有


.. math::

    \mathbb{E}[U(\beta)] &= \mathbb{E} [  (\hat{\beta}-\beta) \mathcal{J}(\hat{\beta})  ]

    &= \mathcal{J}(\hat{\beta})  \mathbb{E}[(\hat{\beta}-\beta) ]

    &= \mathcal{J}(\hat{\beta}) ( \mathbb{E}[\hat{\beta}] -\beta)

    &=0

因此有

.. math::

    \mathbb{E}[\hat{\beta}]= \beta

现在来看下估计量 :math:`\hat{\beta}` 的方差 :math:`V(\hat{\beta})` 。
首先变换下 :eq:`eq_glm_influence_085` 可得

.. math::

    \hat{\beta}-\beta = \frac{U(\beta)}{\mathcal{J}(\hat{\beta})}

然后有

.. math::

    V(\hat{\beta}) &= \mathbb{E} \left [ (\hat{\beta} -\mathbb{E}[\hat{\beta}]) (\hat{\beta}-\mathbb{E}[\hat{\beta}])^T \right ]

    &= \mathbb{E} \left[ (\hat{\beta} -\beta) (\hat{\beta} -\beta)^T \right ]

    &= \mathbb{E} \left[ \mathcal{J}^{-1}U U^T \mathcal{J}^{-1} \right ]

    &= \mathcal{J}^{-1} \mathbb{E} \left[ U U^T  \right ] \mathcal{J}^{-1}

    &= \mathcal{J}^{-1}


最后总结下，参数估计量 :math:`\hat{\beta}` 的期望为参数真值 :math:`\beta`，
方差为 :math:`\mathcal{J}^{-1}`。
结合 :eq:`eq_glm_influence_087`，
参数估计量的抽样分布是正态分布

.. math::

    \hat{\beta} \sim \mathcal{N}(\beta, \mathcal{J}^{-1})


等价的表示是

.. math::
    :label: eq_glm_influence_090

    \frac{ \hat{\beta}-\beta  }{ \sqrt{\mathcal{J}(\hat{\beta})^{-1} } }
    \sim \mathcal{N}(0,1)

如果 :math:`Y` 的分布是正态分布，似然估计量 :math:`\hat{\beta}` 就是精确服从正态分布，而不是渐近了。
如果 :math:`Y` 的分布是非正态分布，似然估计量 :math:`\hat{\beta}` 就是渐近服从正态分布。





参考本节开始时的理论（:eq:`eq_influence_110`）， :eq:`eq_glm_influence_090` 另一个等价的表示是

.. math::
    :label: eq_glm_influence_092

    (\hat{\beta}-\beta)^T\mathcal{J}(\hat{\beta})(\hat{\beta}-\beta) \sim \chi^2(p+1)


:math:`p` 是模型的特征数量，也是协变量参数的数量（不含截距参数），:math:`p+1` 中的 :math:`1` 代表截距参数，
:math:`p+1` 就是模型的参数数量。
:eq:`eq_glm_influence_092` 又叫做 ``Wald`` 统计量。






.. _ch_influence_deviance:

偏差统计量
=================================
现在我们讨论下偏差统计量的抽样分布，
首先回顾一下泰勒展开式 :eq:`eq_influence_250` ，
其中满足 :math:`U(\hat{\beta})=0`，
变化一下公式，则近似有如下等式成立。

.. math::

    \ell(\beta) - \ell(\hat{\beta}) = -\frac{1}{2}(\beta -\hat{\beta} )^T\mathcal{J}(\hat{\beta})(\beta-\hat{\beta})

继续移项，可得到如下统计量

.. math::
    :label: eq_influence_260

    2[\ell(\hat{\beta}) - \ell(\beta) ] = (\hat{\beta} -\beta )^T\mathcal{J}(\hat{\beta})(\hat{\beta}-\beta)

依据 :eq:`eq_glm_influence_092` 这个统计量是服从自由度为 :math:`p+1` 的卡方分布，:math:`p+1`
是模型的参数数量。

.. math::

    2[\ell(\hat{\beta}) - \ell(\beta) ] \sim \chi^2(p+1)

仔细观察下这个统计量，其和偏差的定义基本是一致的。
我知道偏差(deviance) :math:`D`
和对数似然比统计量(log-likelihood ratio statistic)是等价的，其计算公式为

.. math::

    D = 2[\ell(b_{s};y) - \ell(b_{f};y)]

其中，符号 :math:`b_{s}` 表示饱和(𝑠𝑎𝑡𝑢𝑟𝑎𝑡𝑒𝑑)模型参数的最大似然估计量，
:math:`\ell(b_{s};y)` 表示饱和模型的 **似然统计量**。
符号 :math:`b_{f}` 表示我们目标拟合(𝑓𝑖𝑡𝑡𝑒𝑑)模型参数的最大似然估计量，
:math:`\ell(b_{f};y)` 表示拟合模型的 **似然统计量** 。
注意二者是 **统计量(随机变量)**，不是数值量。
参数向量 :math:`b_{s}` 和 :math:`b_{f}` 的长度是不同的，
饱和模型的参数数量就等于样本容量 :math:`N` ，
假设拟合模型的参数向量 :math:`b_{f}` 的长度是 :math:`p+1`， :math:`p+1<N`  。
现在我们把 :math:`D` 变换一下。


.. math::

    D &= 2[\ell(b_{s};y) - \ell(b_{f};y)]
    + 2\ell(\beta_{s};y) - 2\ell(\beta_{s};y)
    + 2\ell(\beta_{f};y)  - 2\ell(\beta_{f};y)

    &= \underbrace{2[ \ell(b_{s};y) -  \ell(\beta_{s};y)  ]}_{\chi^2(N)}
    - \underbrace{2[ \ell(b_{f};y) - \ell(\beta_{f};y)  ]}_{\chi^2(p+1)}
    + \underbrace{2[ \ell(\beta_{s};y) - \ell(\beta_{f};y)   ]}_{\text{数值}v}

其中符号 :math:`\ell(\beta_{s};y)` 表示饱和模型真实参数值的似然值（模型的理论最大似然值），是一个数值，不是统计量。
同理 :math:`\ell(\beta_{f};y)` 是拟合模型的理论最大似然值，也是一个数值。
最终统计量 :math:`D` 可以看做是由三部分组成，自由度为 :math:`N` 卡方分布减去自由度为 :math:`p+1` 的卡方分布，
再加上一个数值 :math:`v` 。

根据卡方分布的特性，统计量 :math:`D` 渐近服从 **非中心卡方分布** ，
其自由度是 :math:`N-p-1` 。

.. math::

    D \sim \chi^2(N-p-1,v)


注意偏差统计量 :math:`D` 是一个 **非中心卡方分布**，这和之前介绍的统计量不同，
:math:`v` 是非中心参数。
:math:`D` 的期望值是 :math:`\mathbb{E}[D] = N-p-1+v` 。
现在来重点看一下 :math:`v` 的值，

.. math::

    v = 2[ \ell(\beta_{s};y) - \ell(\beta_{f};y)   ]

:math:`v` 的值是饱和模型的理论最大似然值和拟合模型的理论最大似然值的差，
前者 :math:`\ell(\beta_{s};y)` 的值是固定不变的，
后者 :math:`\ell(\beta_{f};y)` 是你的拟合模型的理论上限，
拟合模型的拟合效果越好，:math:`\ell(\beta_{f};y)` 就越接近前者饱和模型，
:math:`v` 的值也就越小。
极限情况下，拟合模型对样本的拟合能力和饱和模型一样好，此时 :math:`v=0` 。
这时偏差 :math:`D` 就是渐进服从 **中心卡方分布** :math:`\chi^2(N-p-1)` 。
**本节的内容是下一节的理论基础，对于理解检验过程非常重要。**
**如果难以理解本节的推导过程，可以先记住以下结论。**

.. topic:: 重要结论

    模型对数据拟合的越好(越接近饱和模型)，其偏差 :math:`D` 就越接近中心卡方分布 :math:`\chi^2(N-p-1)` ，
    此时偏差统计量 :math:`D` 的期望就越接近 :math:`N-p-1` 。反之如果模型拟合的不好，偏差统计量 :math:`D`
    就是非中心卡方分布 :math:`\chi^2(N-p-1,v)` ，其期望值就是 :math:`v+N-p-1` ，相比于好的模型期望值会偏离 :math:`N-p-1`
    。后续的比较两个模型效果的假设检验过程就利用这个特性。


如果响应变量 :math:`Y` 是高斯分布，则偏差统计量 :math:`D`
就是确切服从（非中心）卡方分布的，而不是渐近的。
如果响应变量 :math:`Y` 不是高斯分布，则偏差统计量 :math:`D`
是 **渐近** 服从（非中心）卡方分布的。这个特性我们已经多次强调过。


.. hint::

    统计量 :math:`D` 的计算是需要根据对数似然值计算，而对数似然值的计算又需要计算 :math:`V(Y_i)=a(\phi)\nu(\mu_i)`。
    显然要计算对数似然值就需要知道模型的分散参数 :math:`\phi` 的值。指数族中部分分布是没有分散参数 :math:`\phi` 的，
    比如二项分布、多项分布、泊松分布区等，这些模型可以直接计算出统计量 :math:`D` 的值。然而，有些指数族分布，比如高斯分布，
    就存在分散参数 :math:`\phi=\sigma^2` ，此时理论上是无法直接计算出 :math:`D` 的值。这时有两种解决方法，第一种方法是假设
    :math:`\phi` 为一个常量值，传统线性回归模型就是这么干的，其假设 :math:`\phi=\sigma^2=1` 。
    第二种方法就是利用其它估计方法得到 :math:`\phi` 的一个估计值。





``GLM`` 中的模型检验
#########################################

我们已经知道偏差统计量是饱和模型的对数似然值和拟合模型的对数似然值的差，

.. math::

    D = 2[\ell(b_{s};y) - \ell(b_{f};y)]

饱和模型的对数似然值 :math:`\ell(b_{s};y)` 代表了模型似然值的理论最大值，
偏差的含义就是拟合模型的似然值和这个上限值差了多少，偏差越小说明拟合模型对数据的拟合度越好。
理论上偏差 :math:`D` 的取值范围是 :math:`[0,+\infty]`
，然而实际上偏差 :math:`D` 是不大可能得到一个接近0的值。
饱和模型虽然似然值最大，但其是一种极端过拟合(overfitted)的状态，没有学习到任何关于总体的特征，不具备丝毫泛化能力，
**似然值最大并不意味着模型就一定是最好的**。

为了保障模型能从样本数据中学习到总体特征，拟合模型的参数数量 :math:`p` 必然是远小于样本容量 :math:`N` 的，
拟合模型的似然值 :math:`\ell(b_{f};y)` 也必然是远小于饱和模型的似然值 :math:`\ell(b_{s};y)`
，因此偏差 :math:`D` 通常会得到一个比较大的值。并且不同的样本、不同的模型必然会得到不同的 :math:`D` 值，
通常差异也会比较大。那么当你计算出一个 :math:`D` 值时，如何判断模型是好还是坏呢？以及你的结论可靠吗？
毕竟 :math:`D` 是一个统计量(随机量)，仅根据一个值得出结论可信度有多高？
如果你彻底理解了 :numref:`第%s章 <ch_influence_and_test>` 的内容，那么此时你的脑海中应该已经有答案了。



模型检验
=================================



似然比检验
--------------------------------


Wald 检验
--------------------------------




拉格朗日乘子检验
--------------------------------


Lagrange multiplier



F 检验
--------------------------------


我们知道偏差 :math:`D` 是一个统计量，
并且其抽样分布是卡方分布 :math:`\chi^2(N-p,v)` ，期望值是

.. math::

    \mathbb{E}[D] = N-p+v


注意，样本容量 :math:`N` 和模型参数数量 :math:`p` 的值是已知的，
而 :math:`v` 的值我们是无法计算出的。根据之前的结论，模型对数据拟合的越好，:math:`v` 的值就越小，
:math:`D` 的期望值就越接近 :math:`N-p` 。

.. math::

     \mathbb{E}[D] = N-p



那么我们可以基于这个假设对偏差统计量 :math:`D` 进行推断和检验，
如果模型对数据拟合的足够好，则统计量 :math:`D` 的期望值就是
:math:`N-p` ，反之期望值就是 :math:`N-p+v` ，
基于此零假设 :math:`H_0` 和备择假设 :math:`H_1`
分别是

.. math::

    H_0 : \mathbb{E}[D] = N-p

    H_1 : \mathbb{E}[D] \ne N-p


假设显著水平为 :math:`\alpha =0.05`
，然后根据统计量 :math:`D` 的值 :math:`D=d`
计算出 :math:`P-Value` ，
:math:`P` 值就是 :math:`D\ge d` 的概率，
可以通过查卡方检验表直接得到。
通过比较 :math:`P-Value` 和 :math:`\alpha`
得出检验结论。











置信区间
--------------------------------



模型比较
=================================

有些时候我们需要比较两个模型，利用偏差统计量和假设检验可以做到，但是这种方法只适用于嵌套模型。
在 ``GLM`` 中 ，要求两个模型具有相同的指数族分布，以及同样的连接函数，
被比较的两个模型只有线性预测器是不同的，一个参数多，一个参数少，换句话说一个使用的特征多，另一个使用的特征少。
**这种模型比较通常可以用来判断某些特征是否有价值，对模型是否有足够的贡献**。
显然理论上，两个模型参数不同，对数据的拟合度必然会略有不同，
两个模型的偏差统计量的值也必然会有一些差异。通常情况下，参数少的模型偏差会稍大一些。
假设两个模型之间偏差的差值为 :math:`\Delta D`
，那么这个 :math:`\Delta D` 能否证明两个模型对数据的拟合能力有本质的差别，
还是由于随机性导致？假设检验，又或者叫显著性检验，就是用来回答这个问题的。
**显著性检验用于说明** :math:`\Delta D` **能否证明两个模型的拟合能力有"显著性"的差异，**
**当然假设检验并不能给出百分百准确的结论，其只能依概率给出结论**。


在 ``GLM`` 中，检验两个模型拟合能力是否有显著差异的一般性步骤是：

1. 定义模型 :math:`M_0` 对应着零假设 :math:`H_0`，定义另一个更一般(参数更多)的模型 :math:`M_1` 对应着备择假设 :math:`H_a`。
   零假设 :math:`H_0` 表示模型 :math:`M_0` 和 :math:`M_1` 拟合度一样好，反之，
   备择假设 :math:`H_a` 表示  :math:`M_0` 比  :math:`M_1` 拟合度差。
2. 训练模型 :math:`M_0` ，然后计算一个拟合优度(goodness of fit,GOF)指标统计量 :math:`G_0` 。同样训练模型 :math:`M_1` 并计算拟合优度指标 :math:`G_1` 。
3. 计算两个模型拟合度的差异，通常可以是 :math:`\Delta G=G_1-G_0` ，或者是 :math:`\Delta G=G_1/G_0` 。
4. 使用差值统计量 :math:`\Delta G` 的抽样分布检验接受假设 :math:`G_1=G_0` 还是 :math:`G_1 \ne G_0`
5. 如果假设 :math:`G_1=G_0` 没有被拒绝，则接受 :math:`H_0` 。反之，如果假设 :math:`G_1=G_0` 被拒绝，则接受备择假设 :math:`H_a`，
   :math:`M_1` 模型在统计学上显著更优。


现在我们以偏差统计量为例，详细介绍一下比较两个模型的检验过程。
首先我们设定零假设代表模型 :math:`M_0`，模型参数是 :math:`\beta_0`，参数数量为 :math:`q` 。
备择假设代表模型 :math:`M_1` ，模型参数是 :math:`\beta_1`，参数数量为 :math:`p` ，有 :math:`q<p` 。

.. math::

    &H_0: G_0=G_1 \ \text{两个模型拟合效果一样}

    &H_1: G_0 != G_1 \ \text{两个模型拟合效果具有统计学上的显著差异}


我们用 :math:`D_0` 表示模型 :math:`M_0` 的偏差，
用符号 :math:`D_1` 表示模型 :math:`M_1` 的偏差，
两个模型偏差统计量的差值为

.. math::

    \Delta D &= D_0 - D_1

    &= 2[ \ell(b_s;y) - \ell(b_0;y) ] - 2[ \ell(b_s;y) - \ell(b_1;y) ]

    &= 2[  \ell(b_1;y) - \ell(b_0;y)]


我们发现 :math:`\Delta D` 的计算方法和 :math:`D` 的计算方法是一致，都是两个模型对数似然值的差。
根据 :numref:`ch_influence_deviance` 的理论，如果两个模型拟合效果接近，则 :math:`\Delta D`
就渐近服从自由度为 :math:`q-p` 中心卡方分布

.. math::

    \Delta D \sim \chi^2(p-q)

此时 :math:`\Delta D` 的期望值是 :math:`p-q` 。
然而，如果两个模型拟合效果相差较大，则 :math:`\Delta D` 渐近服从非中心卡方分布

.. math::

    \Delta D \sim \chi^2(p-q,v)

此时 :math:`\Delta D` 的期望值是 :math:`p-q+v` ，将会明显大于 :math:`p-q` ，
这个结论将用于对 :math:`H_0` 进行显著性检验。


根据假设检验的过程，我们计算出 :math:`\Delta D` 的值，然后看这个值是否落在
分布 :math:`\chi^2(p-q)` 的拒绝域(比如是否落在图形两端 :math:`100*\alpha \%` 的区域内)
。如果落在拒绝域内，则拒绝 :math:`H_0` 假设，接受 :math:`H_1` 假设。

.. important::

    通常如果两个模型拟合能力相差巨大，:math:`\Delta D` 直观上就很大了，此时也没有进行假设检验的必要了。
    当两个模型的拟合能力比较接近，从经验上(直观上)无法判断 :math:`\Delta D` 是否显著时，才有假设检验的必要。
    此外，相比于直接使用偏差 :math:`D` 做检验，使用统计量 :math:`\Delta D` 进行假设检验更好一些。
    因为 :math:`\Delta D` 通常比单独的偏差 :math:`D` 更加接近中心卡方分布。
    这是因为计算 :math:`D` 的两个模型（饱和模型和拟合模型）的拟合能力差别更大，实际中 :math:`D` 更接近非中心卡方分布。


然而要使用统计量 :math:`\Delta D` 作为检验统计量，这就需要能计算出 :math:`\Delta D` 的值。
前文我们讲过，部分指数族分布存在分散参数 :math:`\phi` ，比如高斯分布，对于这些模型必须知道分散参数 :math:`\phi`
值才能计算出真实的偏差值 :math:`D` ，进而计算出统计量 :math:`\Delta D` 。
虽然我们可以通过一些前提假设解决这个问题，比如传统线性回归(高斯)模型假设 :math:`a(\phi)=\sigma^2=1`，
但这样做会增大 :math:`D` 的计算误差，很可能导致得出错误的检验结论。
针对这个这个问题，我们可以采用另一个检验统计量，F检验统计量，又称F检验。

在 ``GLM`` 中，我们把分散参数 :math:`\phi` 看做是冗余参数，冗余参数的意思是其不再 ``GLM`` 最大似然参数估计的范畴内，
在进行最大似然估计时认为其值是已知。这就需求通弄其它方法来确定冗余参数的值，一般是根据经验进行假设，也可以单独从数据中估计。
回顾下 ``GLM`` 模型一般形式的定义，在定义中，分散参数 :math:`\phi` 与线性预测器 :math:`\eta=\beta^T x` 是独立无关的，
换句话说，两个嵌套模型，拥有同样的 :math:`\phi` ，再结合偏差和尺度化偏差的关系，可得


.. math::

    D_0 &= \frac{d_0}{\phi}

    D_1 &= \frac{d_1}{\phi}

    \Delta D &=  D_0 - D_1 = \frac{d_0-d_1}{\phi}



现在回顾下三大抽样分布中的 :math:`F` 分布，根据 :math:`F` 分布的定义，以下统计量服从 :math:`F` 分布。

.. math::

    F = \left. \frac{\Delta D}{p-q} \middle/ \frac{D_1}{N-p} \right.
    = \left. \frac{d_0 - d_1}{p-q} \middle/ \frac{d_1}{N-p} \right.
    \sim F(p-q,N-P)


:math:`F` 检验统计量可以消除分散参数 :math:`\phi` 的影响。
利用 :math:`F` 统计量检验过程和 :math:`\Delta D` 是一样的，
如果 :math:`H_0` 成立，两个模型拟合效果接近，则 :math:`F` 统计量渐近服从中心分布 :math:`F(p-q,N-P)`。
计算出 :math:`F` 的值，检验其是否落在拒绝域内。

.. important::

    按照 :math:`F` 分布的定义，两个独立的 **中心卡方** 随机变量各自除以自由度后，再相除得到 **中心** :math:`F` 分布。
    一个 **非中心卡方** 随机变量除以一个 **中心卡方** 随机变量得到 **非中心** :math:`F` 分布。
    **这里都要求第二个卡方变量必须是中心卡方变量**，所以要应用 :math:`F` 检验统计量前提是模型 :math:`M_1` 是一个"好的"模型，
    其偏差统计量 :math:`D_1` 是一个中心卡方分布。


正态性检验
================================


案例
####################



:df_model: 模型的自由度，就是特征数量，也是协变量参数的数量 :math:`p-1`（不包含截距参数）
:df_resid: 观测样本数量 :math:`N` 减去协变量参数数量 :math:`p`（包括截距参数）。


线性回归
================================


GLM
================================












笔记
####################



在统计学中，我们需要通过数据的表现去证明假设的成立与否。
如果原假设是成立的，那么其一定会影响到数据的表现，也就是数据一定会受到原假设的影响。
因此最直接的方法就是，找到一个和原假设相关的数据统计量(statistic)，
通过这个统计量的值去验证原假设是否成立。

然而，在统计的世界中，通常我们只能得到一些采样数据，背后隐藏的"真理"是不可知的，
此时只能通过局部采样数据去"猜测"背后的"真理"。
通常我们会用一个随机变量去表示背后的"真理"，采样数据就是这个随机变量的观测样本(observations)。
样本的统计量(statistic)是随机变量样本的函数，不同的观测样本得到不同的统计量值，
因此样本统计量也是一个随机变量，
统计量的概率分布是受到样本所属概率分布的影响的。

我们需要根据样本统计量的值去验证原假设是否成立，
但是统计量也是一个随机变量，它的值也就是随机值，
通常只有统计量取得的某些值时才能证明原假设的成立，
取得其它值时原假设就是不成立的。
既然统计量是个随机变量，我们就需要用概率去描述它，








我们的样本的数据是随机变量的采样值，样本的统计量作为样本的函数也是一个随机量，




假设一个事件为真，作为零假设，它的相反事件



通常这个统计量 抽样分布 是 (近似、渐近)正态分布或者卡方分布


score statistic 服从卡方分布。在 ``GLM`` 中，标准化的 score 统计量是服从正态分布的，其平方是服从卡方分布的

似然估计量是服从正态分布的


偏差是服从卡方分布的


Wald statistic 统计量服从卡方分布


``GLM`` 中假设检验的方法

1. score statistic
2. Wald statistic
3. 偏差统计量 （compare the goodness of fit of two models.）



.. tikz:: An Example Directive with Caption

   \draw[thick,rounded corners=8pt]
   (0,0)--(0,2)--(1,3.25)--(2,2)--(2,0)--(0,2)--(2,2)--(0,0)--(2,0);



An example role :tikz:`[thick] \node[blue,draw] (a) {A};
\node[draw,dotted,right of=a] {B} edge[<-] (a);`


