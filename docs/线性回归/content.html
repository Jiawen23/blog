

<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>6. 线性回归模型 &mdash; Generalized Linear Model 草稿 文档</title>
  

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/translations.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"TeX": {"Macros": {"RR": "{\\bf R}", "bold": ["{\\bf #1}", 1]}}})</script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    
    <link rel="index" title="索引" href="../genindex.html" />
    <link rel="search" title="搜索" href="../search.html" />
    <link rel="next" title="7. 广义线性模型" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html" />
    <link rel="prev" title="5. 指数族" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html" class="icon icon-home" alt="Documentation Home"> Generalized Linear Model
          

          
          </a>

          
            
            
              <div class="version">
                张振虎
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="在文档中搜索" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">目录</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../%E5%89%8D%E8%A8%80/content.html">1. 前言</a></li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html">2. 概率基础</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id2">2.1. 概率模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id3">2.1.1. 概率律</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id4">2.1.2. 离散模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id5">2.1.3. 连续模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id6">2.2. 条件概率</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id7">2.3. 联合概率</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id8">2.4. 全概率与贝叶斯定理</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id9">2.5. 独立性</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id10">2.6. 随机变量</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id11">2.6.1. 离散随机变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id12">2.6.2. 连续随机变量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id13">2.6.3. 累积分布函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id14">2.6.4. 随机变量的函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id15">2.6.5. 期望与方差</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id16">2.7. 边缘化</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id17">2.8. 常见概率分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id18">2.8.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id19">2.8.2. 二项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id20">2.8.3. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id21">2.8.4. 多项式分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id22">2.8.5. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#id23">2.8.6. 卡方分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#t">2.8.7. t分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A6%82%E7%8E%87%E5%9F%BA%E7%A1%80/content.html#f">2.8.8. F分布</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html">3. 最大似然估计</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-liklihood">3.1. 最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id3">3.2. 伯努利分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id4">3.3. 类别分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#ch-2-gaussian-ml">3.4. 高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1/content.html#id6">3.5. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html">4. 推荐与检验</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id2">4.1. 统计量和充分统计量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution">4.2. 抽样分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-sample-distribution-normal">4.2.1. 正态分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#t">4.2.2. t分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id5">4.2.3. 卡方分布</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id6">4.3. 极限理论</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id7">4.3.1. 马尔可夫和切比雪夫不等式</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id8">4.3.2. 弱大数定律</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id9">4.3.3. 依概率收敛</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-clt">4.3.4. 中心极限定理</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id11">4.3.5. 强大数定理</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id12">4.4. 似然估计量</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id13">4.4.1. 估计量的偏差与方差</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-fisher-information">4.4.2. 信息量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-2-mle-estimator">4.4.3. 最大似然估计的特性</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-interval">4.5. 置信区间</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#z">4.5.1. 均值参数的 Z 区间估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id18">4.5.2. 均值参数的 T 区间估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id19">4.5.3. 方差参数的区间估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#ch-influence-test-test">4.6. 简单假设检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id22">4.6.1. Z检验</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id23">4.6.2. T检验</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8E%A8%E6%96%AD%E4%B8%8E%E6%A3%80%E9%AA%8C/content.html#id24">4.6.3. 卡方检验</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html">5. 指数族</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-1">5.1. 指数族的定义</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id3">5.1.1. 伯努利分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id4">5.1.2. 类别分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id5">5.1.3. 泊松分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id6">5.1.4. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id7">5.1.5. 其它常见指数族</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#ch-24-moments">5.2. 指数族的期望与方差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#id9">5.3. 最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html#kl">5.4. 最大似然估计与KL散度的关系</a></li>
</ul>
</li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">6. 线性回归模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#id2">6.1. 最小二乘</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id3">6.1.1. 最小误差</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id4">6.1.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#id5">6.2. 线性回归的概率解释</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#id6">6.2.1. 高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="#id7">6.2.2. 参数估计</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html">7. 广义线性模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id2">7.1. 指数族分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id3">7.1.1. 自然指数族</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id4">7.1.2. 示例：高斯分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id5">7.1.3. 示例：伯努利分布</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id6">7.2. 广义线性模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html#id7">7.3. 例子</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html">8. 参数估计</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate">8.1. 最大似然估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id3">8.2. 泰勒级数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id4">8.3. 梯度下降法</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id6">8.4. 牛顿法</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id7">8.4.1. 算法推导</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id8">8.4.2. 标准连接函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id9">8.4.3. 迭代初始值的设定</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#irls">8.5. 迭代重加权最小二乘(IRLS)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id10">8.5.1. 算法推导</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id11">8.5.2. 算法过程</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#id12">8.6. 估计量的标准误差</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/estimate.html#ch-glm-estimate-phi">8.7. 分散参数的估计</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html">9. 模型评估</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id2">9.1. 拟合优度</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id3">9.1.1. 嵌套模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#likelihood-ratio">9.1.2. 对数似然比(Likelihood ratio)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance">9.1.3. 偏差(deviance)</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#r-2">9.1.4. 决定系数 <span class="math notranslate nohighlight">\(R^2\)</span></a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id4">9.1.4.1. 线性回归中的 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id5">9.1.4.2. 修正的 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#id6">9.1.4.3. 偏差版本的 <span class="math notranslate nohighlight">\(R^2\)</span></a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#ch-glm-gof-chi">9.1.5. 广义皮尔逊卡方统计量</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#residual-analysis">9.2. 残差分析(Residual analysis)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#response-residuals">9.2.1. Response residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#working-residuals">9.2.2. Working residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#partial-residuals">9.2.3. Partial residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#pearson-residuals">9.2.4. Pearson residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#deviance-residuals">9.2.5. Deviance residuals</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#score-residuals">9.2.6. Score residuals</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#model-selection">9.3. 模型选择(model selection)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#aic">9.3.1. AIC</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/content.html#bic">9.3.1.1. BIC</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html">10. 模型检验</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id2">10.1. 拉格朗日乘子检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id3">10.1.1. 得分统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id4">10.1.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#wald">10.2. wald 检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id5">10.2.1. 参数估计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id6">10.2.2. 检验过程</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id7">10.3. 似然比检验</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id8">10.3.1. 抽样分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id9">10.3.2. 模型比较</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id10">10.3.3. 偏差统计量</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#f">10.3.4. F 检验</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%A8%A1%E5%9E%8B%E8%AF%84%E4%BC%B0/influence.html#id11">10.4. 总结</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">11. 高斯模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">11.1. 传统线性回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">11.2. 高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">11.3. 高斯回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">11.4. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">11.4.1. 似然函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">11.4.2. IRLS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">11.4.3. 拟合优度</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id8">11.4.3.1. 偏差统计量</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id9">11.4.3.2. 皮尔逊卡方统计量</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id10">11.5. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html">12. 逆高斯模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id2">12.1. 逆高斯分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id3">12.2. 逆高斯回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id4">12.3. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id5">12.3.1. 似然函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#irls">12.3.2. IRLS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id6">12.3.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%80%86%E9%AB%98%E6%96%AF%E6%A8%A1%E5%9E%8B/content.html#id7">12.4. 其它连接函数</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">13. 二项式模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">13.1. 伯努利分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">13.2. 逻辑回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">13.2.1. 模型定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">13.2.2. 参数估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#odds-logit">13.2.3. odds 与 logit</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">13.3. 二项式分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">13.4. 二项式回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">13.4.1. 模型定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">13.4.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">13.5. 其它连接函数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">13.5.1. 恒等连接函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#probit">13.5.2. probit 回归</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#log-log-clog-log">13.5.3. log-log 和 clog-log</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">13.6. 分组数据与比例数据</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html">14. 泊松模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#poisson">14.1. 泊松(Poisson)分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id2">14.1.1. 推导过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id3">14.1.2. 泊松分布的特性</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id4">14.1.2.1. 分布的矩</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id5">14.2. 泊松回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id6">14.3. 参数估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id7">14.4. 拟合统计量</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id8">14.5. 频率模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%B3%8A%E6%9D%BE%E6%A8%A1%E5%9E%8B/content.html#id9">14.6. 泊松模型的局限性</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html">15. 指数模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#exponential">15.1. 指数(exponential)分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id2">15.1.1. 推导过程</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id3">15.1.2. 分布的特性</a><ul>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id4">15.1.2.1. 曲线图</a></li>
<li class="toctree-l4"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id5">15.1.2.2. 分布的矩</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id6">15.2. 指数回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id7">15.3. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id8">15.3.1. 似然函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#id9">15.3.2. 拟合优度</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%8C%87%E6%95%B0%E6%A8%A1%E5%9E%8B/content.html#irls">15.3.3. IRLS</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html">16. Gamma 模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id1">16.1. Gamma 函数</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id2">16.2. Gamma 分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id3">16.3. Gamma 回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id4">16.4. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id5">16.4.1. 似然函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#irls">16.4.2. IRLS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id6">16.4.3. 拟合优度</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id7">16.5. 其他连接函数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#id8">16.5.1. 对数 Gamma 模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../gamma%E6%A8%A1%E5%9E%8B/content.html#identity-gamma">16.5.2. 恒等(identity) Gamma 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html">17. 过度分散</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id2">17.1. 什么是过度分散</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id3">17.2. 过度分散的检测</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id4">17.3. 过度分散的影响</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%BF%87%E5%BA%A6%E5%88%86%E6%95%A3/content.html#id5">17.4. 标准误差的修正</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html">18. 负二项式模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id2">18.1. 负二项式分布</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id3">18.1.1. 从二项式分布推导</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id4">18.1.2. 泊松-伽马混合分布</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#alpha">18.1.3. 辅助参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的影响</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id5">18.2. 负二项回归模型</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id6">18.3. 参数估计</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#irls">18.3.1. IRLS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id7">18.3.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id8">18.4. 负二项式模型扩展</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id9">18.4.1. 对数连接函数</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id10">18.4.2. 参数 <span class="math notranslate nohighlight">\(\alpha\)</span> 的估计</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id11">18.4.3. 几何模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E8%B4%9F%E4%BA%8C%E9%A1%B9%E6%A8%A1%E5%9E%8B/content.html#id12">18.4.4. 广义负二项式模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html">19. 零计数问题</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id2">19.1. 零截断模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id3">19.1.1. 零截断泊松模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id4">19.1.2. 零截断负二项式模型</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#id5">19.2. 零膨胀模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#hurdle">19.2.1. Hurdle 模型</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E9%9B%B6%E8%AE%A1%E6%95%B0%E9%97%AE%E9%A2%98/content.html#zero-inflate">19.2.2. Zero-inflate 模型</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">20. 多项式模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">20.1. 类别分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#softmax">20.2. softmax 回归模型</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">20.2.1. 模型定义</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">20.2.2. 参数估计</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">20.3. 多项式分布</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%97%A0%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id6">20.4. 多项式回归模型</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html">21. 有序离散模型</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id2">21.1. 有序逻辑回归</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id3">21.2. 参数估计</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id4">21.3. 连接函数</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#logit">21.3.1. logit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#probit">21.3.2. probit</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#clog-log">21.3.3. clog-log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#log-log">21.3.4. log-log</a></li>
<li class="toctree-l3"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#cauchit">21.3.5. cauchit</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../%E6%9C%89%E5%BA%8F%E7%A6%BB%E6%95%A3%E6%A8%A1%E5%9E%8B/content.html#id5">21.4. 总结</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html">附录</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id2">标准正态累积分布表</a></li>
<li class="toctree-l2"><a class="reference internal" href="../%E9%99%84%E5%BD%95/%E5%88%86%E5%B8%83%E8%A1%A8.html#id3">卡方分布临界值表</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE/content.html">参考文献</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">Generalized Linear Model</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li><span class="section-number">6. </span>线性回归模型</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/线性回归/content.rst.txt" rel="nofollow"> 查看页面源码</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="ch-29">
<span id="id1"></span><h1><span class="section-number">6. </span>线性回归模型<a class="headerlink" href="#ch-29" title="永久链接至标题">¶</a></h1>
<p>在统计分析中，经常需要分析变量之间的关系，
最常见的场景就是研究某个变量如何取决于其它一个或多个变量，
通常会把研究的目标变量称为响应变量、输出变量、被解释变量，
把影响目标变量的其它变量称为预测变量、输入变量、解释变量等。
习惯上，用符号 <span class="math notranslate nohighlight">\(X\)</span> 表示输入变量，
用符号 <span class="math notranslate nohighlight">\(Y\)</span>  表示输出变量，
通常二者间存在一定的关系
，统计学家研究内容就是找到二者之间合适的函数关系，
<span class="math notranslate nohighlight">\(y=f(x)\)</span>
，然而二者之间最真实的关系是不得知的，
统计学家也只是尽可能的找到一个近似的关系。
线性回归模型是统计学中最基本的模型，
也是机器学习领域的入门模型，
得益于它的简单，应用十分广泛。
本书的主题，广义线性模型就是线性回归模型的扩展，
在讨论广义线性模型之前，先简单回顾一下线性回归模型。</p>
<div class="section" id="id2">
<h2><span class="section-number">6.1. </span>最小二乘<a class="headerlink" href="#id2" title="永久链接至标题">¶</a></h2>
<div class="section" id="id3">
<h3><span class="section-number">6.1.1. </span>最小误差<a class="headerlink" href="#id3" title="永久链接至标题">¶</a></h3>
<p>通常情况下，机器学习就是要找到输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 和输入变量 <span class="math notranslate nohighlight">\(X\)</span> 的函数关系，
变量之间最简单的函数关系，就是线性函数关系。
在线性回归模型中，假设 <span class="math notranslate nohighlight">\(Y\)</span> 与 <span class="math notranslate nohighlight">\(X\)</span> 之间是一个线性函数的关系。</p>
<div class="math notranslate nohighlight" id="equation-eq-29-1">
<span class="eqno">(6.1.1)<a class="headerlink" href="#equation-eq-29-1" title="公式的永久链接">¶</a></span>\[y = \beta_0+ \beta_1 x_1 +\beta_2 x_2+\dots+ \beta_p x_p\]</div>
<p>由于输入特征数据 <span class="math notranslate nohighlight">\(X\)</span> 通常是多维的，所以 <span class="math notranslate nohighlight">\(f(x)\)</span> 是一个多元一次函数，
其中 <span class="math notranslate nohighlight">\(x_j\)</span> 是第 <span class="math notranslate nohighlight">\(j\)</span> 维输入数据，<span class="math notranslate nohighlight">\(x_j\)</span> 可以是任意实数值。
<span class="math notranslate nohighlight">\(\beta_j (j &gt; 0)\)</span> 是 <span class="math notranslate nohighlight">\(x_j\)</span> 的系数，<span class="math notranslate nohighlight">\(\beta_0\)</span> 是这个线性方程的截距，
我们把 <span class="math notranslate nohighlight">\(\beta_j\)</span> 看做这个函数的未知参数，其值暂时是未知的。
<a class="reference internal" href="#equation-eq-29-1">公式(6.1.1)</a> 看上去不是很简洁，通常为了公式的简洁表达，我们令
<span class="math notranslate nohighlight">\(\beta^T=[\beta_0,\beta_1,\dots,\beta_p],x^T=[1,x_1,x_2,\dots,x_p]\)</span> ，
然后把 <a class="reference internal" href="#equation-eq-29-1">公式(6.1.1)</a> 看成是特征向量 <span class="math notranslate nohighlight">\(x\)</span> 和参数向量 <span class="math notranslate nohighlight">\(\beta\)</span> 的內积形式。
线性回归模型的简洁表示为：</p>
<div class="math notranslate nohighlight" id="equation-content-0">
<span class="eqno">(6.1.2)<a class="headerlink" href="#equation-content-0" title="公式的永久链接">¶</a></span>\[y =  \beta_0 \times 1 + \beta_1x_1 +\beta_2 x_2+\dots+\beta_p x_p
= x^T \beta\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>为了满足向量內积的形式，我们人为的增加一列特征数据 <span class="math notranslate nohighlight">\(x_0=1\)</span>，所有输入样本的 <span class="math notranslate nohighlight">\(x_0\)</span> 都为常量1。</p>
</div>
<p><a class="reference internal" href="#fg-29-1"><span class="std std-numref">图 6.1.1</span></a> 是单一维度输入变量的线性回归模型的图形化展示，
为了方便图形化展示，我们假设输入特征变量 <span class="math notranslate nohighlight">\(X\)</span> 只有一维。
图中蓝色的点代表一些 <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> 的样本点，下标 <span class="math notranslate nohighlight">\(i\)</span> 是样本点的编号。
线性回归模型的本质就是找到一条直线 <span class="math notranslate nohighlight">\(y=x^T \beta\)</span> (比如图中的红色直线)
，并且这条直线和样本点的”走势”是一致的，这样我们就可以用这条直线去预测新的样。
比如根据图中蓝色样本点的分布，我们找到红色直线和样本的”走势”一致，
这样当输入一个新的 <span class="math notranslate nohighlight">\(x_{new}\)</span> 时，就用直线上的点 <span class="math notranslate nohighlight">\(y_{new}=x_{new}^T \beta\)</span>
作为预测值 <span class="math notranslate nohighlight">\(\hat{y}=y_{new}=x_{new}^T \beta\)</span> 。</p>
<div class="figure align-center" id="id8">
<span id="fg-29-1"></span><a class="reference internal image-reference" href="../_images/29_1.png"><img alt="../_images/29_1.png" src="../_images/29_1.png" style="width: 320.0px; height: 240.0px;" /></a>
<p class="caption"><span class="caption-number">图 6.1.1 </span><span class="caption-text">单一输入特征的回归模型</span><a class="headerlink" href="#id8" title="永久链接至图片">¶</a></p>
</div>
<p>然而空间上存在无数条直线，要如何确定和样本点”走势”相同的直线呢？
我们的最终目的是用这条之间预测新的样本点，那么理论上预测最准的直线是最优的直线。
对于一条样本数据 <span class="math notranslate nohighlight">\((x_i,y_i)\)</span> ，模型的预测值是 <span class="math notranslate nohighlight">\(\hat{y}_i = x_i^T \beta\)</span>
,显然，最优的直线就是 <strong>预测误差</strong> 最小的直线。
我们把样本的真实值 <span class="math notranslate nohighlight">\(y_i\)</span> 和预测值 <span class="math notranslate nohighlight">\(\hat{y}_i\)</span> 之间的差值定义成残差(residual)，
我们的目标就是找到一条令所有观测样本残差最小的直线。
通常我们使用所有样本残差的平方和(residual sum of squares,RSS)做为整体的误差。</p>
<div class="math notranslate nohighlight" id="equation-content-1">
<span class="eqno">(6.1.3)<a class="headerlink" href="#equation-content-1" title="公式的永久链接">¶</a></span>\[J(\beta)=\sum_{i=1}^N (\hat{y}_i-y_i)^2\]</div>
<div class="figure align-center" id="id9">
<span id="fg-29-2"></span><a class="reference internal image-reference" href="../_images/29_2.png"><img alt="../_images/29_2.png" src="../_images/29_2.png" style="width: 279.0px; height: 216.5px;" /></a>
<p class="caption"><span class="caption-number">图 6.1.2 </span><span class="caption-text">线性回归的残差</span><a class="headerlink" href="#id9" title="永久链接至图片">¶</a></p>
</div>
<p>我们认为令 <code class="docutils literal notranslate"><span class="pre">RSS</span></code> 取得最小值的直线的最优的直线，
所以我们通过极小化 <code class="docutils literal notranslate"><span class="pre">RSS</span></code> 来确定这条最优的直线，
由于直线是由参数 <span class="math notranslate nohighlight">\(\beta\)</span> 决定的，所以要确定这条直线就是等价于找到参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的值。
因此：</p>
<div class="math notranslate nohighlight" id="equation-content-2">
<span class="eqno">(6.1.4)<a class="headerlink" href="#equation-content-2" title="公式的永久链接">¶</a></span>\[\hat{\beta} = \mathop{\arg \max}_{\beta} \sum_{i=1}^N (\hat{y}_i - y_i)^2
=\mathop{\arg \max}_{\beta} \sum_{i=1}^N (x_i^T \beta-y_i)^2\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>通常机器学习的过程，都是先根据场景或数据定义一个参数化的模型函数 <span class="math notranslate nohighlight">\(y=f(x,\beta)\)</span>
，模型函数是对单条数据样本的建模。但它是含有未知参数的，
我们需要找到一个最优的参数值使得这个模型函数尽可能好的拟合数据样本。
因此就需要定义一个评价不同参数值模型好坏的标准或者说函数，通常我们称这个评价函数为目标函数(object function)，
然后通过极大(小)化目标函数求得参数的最优解。
比如似然函数就是目标函数的一种，除此之外，还可以定义某种损失(误差)函数(cost function| loss function | error function)
作为目标函数，比如线性回归的平方损失、逻辑回归的交叉熵损失等等。</p>
</div>
<p><strong>以“残差平方和最小”确定直线位置的方法被称为最小二乘法。</strong>
用最小二乘法除了计算比较方便外，得到的估计量还具有优良特性， <strong>这种方法对异常值非常敏感</strong>。</p>
</div>
<div class="section" id="id4">
<h3><span class="section-number">6.1.2. </span>参数估计<a class="headerlink" href="#id4" title="永久链接至标题">¶</a></h3>
<p>显然对于线性回归模型， <code class="docutils literal notranslate"><span class="pre">RSS</span></code> 是一个关于 <span class="math notranslate nohighlight">\(\beta\)</span> 的二次函数，
我们知道二次函数一定是存在唯一的一个极值点的，所以参数 <span class="math notranslate nohighlight">\(\beta\)</span> 一定存在唯一解，
并且在极值点函数的导数为 <span class="math notranslate nohighlight">\(0\)</span>。
所以我们可以直接求出 <code class="docutils literal notranslate"><span class="pre">RSS</span></code> 的导数，并令导数为 <span class="math notranslate nohighlight">\(0\)</span> 的方法求得 <span class="math notranslate nohighlight">\(\hat{\beta}\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-content-3">
<span class="eqno">(6.1.5)<a class="headerlink" href="#equation-content-3" title="公式的永久链接">¶</a></span>\[\frac{\partial J}{\partial \beta} = \sum_{i=1}^N 2 x_i (x_i^T \beta - y_i)\]</div>
<p>上述偏导结果中包含所有样本的求和符号 <span class="math notranslate nohighlight">\(\sum_{i=1}^N\)</span> ，为了简单表达我们用矩阵和向量乘积的方式替换求和符号。
我们用符号 <span class="math notranslate nohighlight">\(X\)</span> 表示训练样本集中所有的输入数据的矩阵，
用符号 <span class="math notranslate nohighlight">\(y\)</span> 表示训练样本集中所有输出数据的向量。上述偏导用矩阵符号表示为：</p>
<div class="math notranslate nohighlight" id="equation-content-4">
<span class="eqno">(6.1.6)<a class="headerlink" href="#equation-content-4" title="公式的永久链接">¶</a></span>\[\frac{\partial J}{\partial \beta} = 2X^T (X \beta - y)\]</div>
<p>然后我们令导数为0，可以得到：</p>
<div class="math notranslate nohighlight" id="equation-eq-29-03">
<span class="eqno">(6.1.7)<a class="headerlink" href="#equation-eq-29-03" title="公式的永久链接">¶</a></span>\[X^TX\beta = X^T y\]</div>
<p><a class="reference internal" href="#equation-eq-29-03">公式(6.1.7)</a> 通常被称为正规方程组(normal equations)，
理论上，我们可以根据这个等式得到参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的解析解</p>
<div class="math notranslate nohighlight" id="equation-content-5">
<span class="eqno">(6.1.8)<a class="headerlink" href="#equation-content-5" title="公式的永久链接">¶</a></span>\[\hat{\beta}=(X^TX)^{-1} X^Ty\]</div>
<p>然而，这个解析解需要求矩阵 <span class="math notranslate nohighlight">\(X^TX\)</span> 的逆矩阵，
矩阵存在逆矩阵需要满足两个条件：(1)是方阵，(2)是满秩的。
虽然是 <span class="math notranslate nohighlight">\(X^TX\)</span> 方阵，但是未必满秩，
那么也就不存在逆矩阵，当逆矩阵不存在时也没没办法计算解析解。
当矩阵 <span class="math notranslate nohighlight">\(X\)</span> 存在(行或列)共线性时，<span class="math notranslate nohighlight">\((X^TX)\)</span> 一定是不满秩的。</p>
<p><span class="math notranslate nohighlight">\((X^TX)\)</span> 不存在逆矩阵不代表参数 <span class="math notranslate nohighlight">\(\beta\)</span> 无解，上面已经讲过损失函数 <span class="math notranslate nohighlight">\(J(\beta)\)</span>
是二次函数，一定存在唯一的极值点，所以参数 <span class="math notranslate nohighlight">\(\beta\)</span> 一定有全局最优解的。
当无法求得解析解时，我们可以使用迭代法求解，比如基于一阶导数的梯度下降法和基于二阶导数的牛顿法。</p>
</div>
</div>
<div class="section" id="id5">
<h2><span class="section-number">6.2. </span>线性回归的概率解释<a class="headerlink" href="#id5" title="永久链接至标题">¶</a></h2>
<p>现在我们尝试在概率的框架下解释线性回归模型。
在概率的框架下，认为输入变量 <span class="math notranslate nohighlight">\(X\)</span> 和输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 都是随机变量，
两个变量的联合概率为</p>
<div class="math notranslate nohighlight" id="equation-content-6">
<span class="eqno">(6.2.1)<a class="headerlink" href="#equation-content-6" title="公式的永久链接">¶</a></span>\[P(X,Y)=P(X)P(Y|X)\]</div>
<p>在回归问题中，是给定输入 <span class="math notranslate nohighlight">\(x\)</span> ，模型输出 <span class="math notranslate nohighlight">\(y\)</span> 的值，特征变量 <span class="math notranslate nohighlight">\(X\)</span> 的值是已知的确定的，
所以不需要边缘概率 <span class="math notranslate nohighlight">\(P(X)\)</span> ，只需要得到条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 即可。
换句话说，不需要对联合概率 <span class="math notranslate nohighlight">\(P(X,Y)\)</span> 进行建模，只需要建模条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span>
。在概率的框架下，用条件概率 <span class="math notranslate nohighlight">\(P(Y|X)=f(x)\)</span> 去表达这种关系，
即当 <span class="math notranslate nohighlight">\(X=x\)</span> 时，变量 <span class="math notranslate nohighlight">\(Y=y\)</span> 的概率为 <span class="math notranslate nohighlight">\(P(Y=y|X=x)\)</span> 。</p>
<div class="section" id="id6">
<h3><span class="section-number">6.2.1. </span>高斯分布<a class="headerlink" href="#id6" title="永久链接至标题">¶</a></h3>
<div class="figure align-center" id="id10">
<span id="fg-29-12"></span><a class="reference internal image-reference" href="../_images/29_12.jpg"><img alt="../_images/29_12.jpg" src="../_images/29_12.jpg" style="width: 296.7px; height: 203.7px;" /></a>
<p class="caption"><span class="caption-number">图 6.2.1 </span><span class="caption-text">线性回归模型表示成条件均值函数加上一个高斯噪声。</span><a class="headerlink" href="#id10" title="永久链接至图片">¶</a></p>
</div>
<p>观察 <a class="reference internal" href="#fg-29-12"><span class="std std-numref">图 6.2.1</span></a>
，变量 <span class="math notranslate nohighlight">\(Y\)</span> 的值，可以看做是线性预测器 <span class="math notranslate nohighlight">\(x^T \beta\)</span> 的值加上一个误差项（噪声）
<span class="math notranslate nohighlight">\(\epsilon\)</span> 。</p>
<div class="math notranslate nohighlight" id="equation-content-7">
<span class="eqno">(6.2.2)<a class="headerlink" href="#equation-content-7" title="公式的永久链接">¶</a></span>\[y=x^T \beta + \epsilon\]</div>
<p>对于噪声，最普遍常用的就是高斯噪声，
这里假设 <span class="math notranslate nohighlight">\(\epsilon\)</span> 是一个均值为 <span class="math notranslate nohighlight">\(0\)</span>
，方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 的高斯噪声，
<span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0,\sigma^2)\)</span>
。线性部分 <span class="math notranslate nohighlight">\(x^T \beta\)</span>
是一个数值量，不是随机量。
期望为 <span class="math notranslate nohighlight">\(0\)</span> 的高斯项 <span class="math notranslate nohighlight">\(\epsilon\)</span> 加上一个数值项 <span class="math notranslate nohighlight">\(x^T \beta\)</span>
得到的就是一个期望值为 <span class="math notranslate nohighlight">\(x^T \beta\)</span> 随机变量，
因此输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 是一个高斯变量。</p>
<div class="math notranslate nohighlight" id="equation-content-8">
<span class="eqno">(6.2.3)<a class="headerlink" href="#equation-content-8" title="公式的永久链接">¶</a></span>\[Y \sim \mathcal{N}(x^T \beta,\sigma^2)\]</div>
<p>因此条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 是均值为 <span class="math notranslate nohighlight">\(x^T \beta\)</span> 方差为 <span class="math notranslate nohighlight">\(\sigma^2\)</span>
的高斯分布，
条件概率分布 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 的概率密度函数为：</p>
<div class="math notranslate nohighlight" id="equation-eq-29-10">
<span class="eqno">(6.2.4)<a class="headerlink" href="#equation-eq-29-10" title="公式的永久链接">¶</a></span>\[p(y|x;\beta) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \{ -\frac{1}{2\sigma^2}(y-\beta^Tx)^2 \}\]</div>
<div class="admonition note">
<p class="admonition-title">注解</p>
<p>实际上 <span class="math notranslate nohighlight">\(Y\)</span> 的概率分布要根据数据的实际分布确定，并不是一定要高斯分布。
只不过在线性回归这个模型中我们假设 <span class="math notranslate nohighlight">\(Y\)</span> 是服从高斯分布的。
如果你的数据不是(近似)高斯分布，那么就不应该使用线性回归模型。
在后面的章节中我们会介绍当 <span class="math notranslate nohighlight">\(Y\)</span> 是其它分布时，应该怎么处理，
在广义线性模型的框架下，输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 可以扩展到指数族分布。</p>
</div>
<p>在传统线性回归模型中，方差项 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 被假设为常量 <span class="math notranslate nohighlight">\(1\)</span> 。
<span class="math notranslate nohighlight">\(\beta\)</span> 是模型的参数，是需要利用观测数据进行学习的。
高斯版的线性回归模型，是一个概率模型，可以使用最大似然估计参数 <span class="math notranslate nohighlight">\(\beta\)</span>
，下一节详细阐述如何利用最大似然估计参数 <span class="math notranslate nohighlight">\(\beta\)</span> 。</p>
<p>在得到参数 <span class="math notranslate nohighlight">\(\beta\)</span> 的最大似然估计值 <span class="math notranslate nohighlight">\(\hat{\beta}_{ML}\)</span>
后，可以用高斯变量 <span class="math notranslate nohighlight">\(Y\)</span> 的期望值作为模型的预测值，
模型输入一个 <span class="math notranslate nohighlight">\(x\)</span> 值，输出
<span class="math notranslate nohighlight">\(\hat{y}=\mathbb{E}[P(Y|X=x;\hat{\beta}_{ML})]=x^T \hat{\beta}_{ML}\)</span>
。</p>
<p>实际上，线性回归模型比我们看上去的要广泛，线性组合部分 <span class="math notranslate nohighlight">\(x^T \beta\)</span> 只要求对 <span class="math notranslate nohighlight">\(\beta\)</span> 是线性的，
并不要求对 <span class="math notranslate nohighlight">\(x\)</span> 是线性的，所以可以为 <span class="math notranslate nohighlight">\(x\)</span> 加上一个非线性的函数 <span class="math notranslate nohighlight">\(\phi(\cdot)\)</span>
使模型具有拟合非线性数据的能力。</p>
<div class="math notranslate nohighlight" id="equation-content-9">
<span class="eqno">(6.2.5)<a class="headerlink" href="#equation-content-9" title="公式的永久链接">¶</a></span>\[y=\beta^T \phi(x) + \epsilon =\beta^T x' + \epsilon\]</div>
<p><span class="math notranslate nohighlight">\(\phi(x)\)</span> 可以看做是对特征数据的预处理 <span class="math notranslate nohighlight">\(x \Rightarrow \phi(x)\)</span>
，转化之后的 <span class="math notranslate nohighlight">\(x'=\phi(x)\)</span> 作为模型的输入特征，并不影响线性回归模型的定义和计算。</p>
<p>高斯假设的线性回归模型是建立在两个很强的假设之上的，(1)条件概率 <span class="math notranslate nohighlight">\(P(Y|X)\)</span> 是高斯分布，
并且(2)不同的 <span class="math notranslate nohighlight">\(x\)</span> 条件下方差是相同的。然而这种假设在很多时候是不满足的，尤其是第(2)点，
这也是线性回归的模型的局限性。</p>
</div>
<div class="section" id="id7">
<h3><span class="section-number">6.2.2. </span>参数估计<a class="headerlink" href="#id7" title="永久链接至标题">¶</a></h3>
<p>假设我们有一个成对的观测数据集 <span class="math notranslate nohighlight">\(\mathcal{D}=\{(x_i,y_i);i=1,2,\dots,N\}\)</span>
，其中 <span class="math notranslate nohighlight">\(x_i\)</span> 是一条输入变量 <span class="math notranslate nohighlight">\(X\)</span> 的观测样值，<span class="math notranslate nohighlight">\(y_i\)</span> 是对应的输出变量 <span class="math notranslate nohighlight">\(Y\)</span> 的观测值，
注意 <span class="math notranslate nohighlight">\(x_i\)</span> 是一个 <span class="math notranslate nohighlight">\(p\)</span> 维的向量(vector)，而 <span class="math notranslate nohighlight">\(y_i\)</span> 是一个标量(scalar)。
样本集中的样本都是满足独立同分布(IID)的。
样本集的联合概率可以写成：</p>
<div class="math notranslate nohighlight" id="equation-content-10">
<span class="eqno">(6.2.6)<a class="headerlink" href="#equation-content-10" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}P(\mathcal{D}) &amp;= \prod_{i=1}^N P(y_i|x_i;\beta)\\&amp;=  {2\pi\sigma^2}^{-\frac{N}{2}} \exp \{ -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-x_i^T \beta)^2 \}\end{aligned}\end{align} \]</div>
<p>我们知道观测样本集的联合概率就是似然函数，我们可以通过最大似然估计法估计出模型的未知参数 <span class="math notranslate nohighlight">\(\beta\)</span> ，
为了计算简单，通常我们采用极大化对数似然函数的方法估计参数。
线性回归模型的对数似然函数为：</p>
<div class="math notranslate nohighlight" id="equation-content-11">
<span class="eqno">(6.2.7)<a class="headerlink" href="#equation-content-11" title="公式的永久链接">¶</a></span>\[\ell(\beta;x,y) = \underbrace{{-\frac{N}{2}} \ln (2\pi\sigma^2)}_{\text{常量}}
-\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-x_i^T\beta)^2\]</div>
<p>由于我们假设方差 <span class="math notranslate nohighlight">\(\sigma^2\)</span> 是常量，所以上述公式的第一项是一个常量，
在极大化对数似然函数时不影响最终的求解，所以是可以去掉的。</p>
<div class="math notranslate nohighlight" id="equation-content-12">
<span class="eqno">(6.2.8)<a class="headerlink" href="#equation-content-12" title="公式的永久链接">¶</a></span>\[ \begin{align}\begin{aligned}\hat{\beta}_{ML} &amp;={\arg \max}_{\beta}  \ell(\beta;x,y)\\&amp;\triangleq  {\arg \max}_{\beta}
\left \{ -\frac{1}{2\sigma^2} \sum_{i=1}^N (y_i-x_i^T \beta)^2
\right \}\end{aligned}\end{align} \]</div>
<p>我们发现这和最小二乘法的损失函数 <span class="math notranslate nohighlight">\(J(\beta)\)</span> 是等价的，
对数似然函数中的 <span class="math notranslate nohighlight">\(\sum_{i=1}^N (y_i-x_i^T \beta)^2\)</span> 就是残差的平方和(residual sum of squares,RSS)。
同理，我们可以使用迭代法求的最优解。</p>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../%E5%B9%BF%E4%B9%89%E7%BA%BF%E6%80%A7%E6%A8%A1%E5%9E%8B/content.html" class="btn btn-neutral float-right" title="7. 广义线性模型" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="../%E6%8C%87%E6%95%B0%E6%97%8F/content.html" class="btn btn-neutral float-left" title="5. 指数族" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> 上一页</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; 版权所有 2019, zhangzhenhu(acmtiger@outlook.com).

    </p>
  </div>
    
    
    
    利用 <a href="http://sphinx-doc.org/">Sphinx</a> 构建，使用了 
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">主题</a>
    
    由 <a href="https://readthedocs.org">Read the Docs</a>开发.
    <script src="https://utteranc.es/client.js"
            repo="zhangzhenhu/blog"
            issue-term="pathname"
            theme="github-light"
            crossorigin="anonymous"
            async>
    </script>



</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>